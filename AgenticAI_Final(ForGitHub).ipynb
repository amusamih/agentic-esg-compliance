{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LC1sg4J3jAGx",
        "FcGjvSGdjE9M",
        "F3es7xvijJMN",
        "Bsh8zo1zuDSF",
        "IJKpIulxepqI",
        "fznMEI25JBaz",
        "QdwVrmTVHLB5",
        "Ufpb-UGFnSfc",
        "8vDcol_qQEK2",
        "sm3Hi7Ynv_w7",
        "4v7mzEeb9c4N"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "LC1sg4J3jAGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENVIRONMENT SETUP ===\n",
        "!pip install -U langchain langchain-core langchain-community langchain-openai\n",
        "!pip install -q openai tiktoken sentence-transformers faiss-cpu unstructured pymupdf pdf2image pytesseract pillow json5\n",
        "!apt-get install -y poppler-utils tesseract-ocr\n",
        "!pip install ipywidgets\n",
        "!pip install -q docling pymupdf\n",
        "!pip install -q anthropic\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "\n",
        "# === CORE LIBRARIES ===\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import json5\n",
        "import time\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from IPython.display import FileLink\n",
        "\n",
        "# === COLAB UTILITIES ===\n",
        "from google.colab import files\n",
        "\n",
        "# === PDF + TEXT PARSING ===\n",
        "from tqdm import tqdm\n",
        "from docling.document_converter import DocumentConverter\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.document_converter import PdfFormatOption, InputFormat\n",
        "from docling.backend.docling_parse_v2_backend import DoclingParseV2DocumentBackend\n",
        "\n",
        "# === LANGCHAIN CORE ===\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.agents import tool\n",
        "\n",
        "# === LLM PROVIDERS ===\n",
        "from openai import OpenAI\n",
        "import anthropic\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "# === GLOBAL STATE (User Session Variables) ===\n",
        "full_text = \"\"\n",
        "ESG_CHUNKS = []\n",
        "ESG_VECTORSTORE = None\n",
        "GRI_ID_MAP = {}\n",
        "GRI_DEPRECATED_IDS = set()\n",
        "DISCLOSURES = []\n",
        "MATCH_RESULTS = []\n",
        "SELECTED_MODELS = []\n",
        "METADATA_BY_MODEL = {}\n",
        "EVAL_RESULTS = {}\n",
        "FIX_RESULTS = {}\n",
        "SESSION_LOG = []\n",
        "GLOBAL_STATE_INDEX = None\n",
        "RESULTS = {\n",
        "    \"matching\": [],\n",
        "    \"metadata\": {},\n",
        "    \"evaluation\": {},\n",
        "    \"fixes\": {}\n",
        "}\n",
        "\n",
        "\n",
        "# === DEFAULT MODEL PROVIDERS ===\n",
        "providers = {\n",
        "    \"gpt\": {\"provider\": \"openai\", \"model\": \"gpt-4.1-nano\"},\n",
        "    \"claude\": {\"provider\": \"claude\", \"model\": \"claude-3-5-haiku-20241022\"},\n",
        "    \"gemini\": {\"provider\": \"gemini\", \"model\": \"gemini-2.0-flash-lite\"}\n",
        "}\n",
        "\n",
        "EMBED_MODEL = \"text-embedding-3-large\"\n"
      ],
      "metadata": {
        "id": "xdQlFqjZkih7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"ENTER_YOUR_API_HERE\"\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"ENTER_YOUR_API_HERE\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"ENTER_YOUR_API_HERE\""
      ],
      "metadata": {
        "id": "mVRVLkgJspNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "def build_global_state_index(docs, rebuild=False):\n",
        "    FAISS_INDEX_PATH = \"faiss_global_index\"\n",
        "    embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "    if os.path.exists(FAISS_INDEX_PATH) and not rebuild:\n",
        "        print(\"✅ Loading saved FAISS index...\")\n",
        "        return FAISS.load_local(FAISS_INDEX_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "    print(\"📦 Building FAISS index from scratch...\")\n",
        "    index = FAISS.from_documents(docs, embedding_model)\n",
        "    index.save_local(FAISS_INDEX_PATH)\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "1R0GeCSJVaft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output files upload for analysis**"
      ],
      "metadata": {
        "id": "AqmpcJ_BbOJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload e.g., all_outputs.zip\n"
      ],
      "metadata": {
        "id": "vFCC_2qJbKQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "with zipfile.ZipFile(\"content_directory_backup.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Confirm\n",
        "print(\"✅ Extracted folders:\", os.listdir())\n"
      ],
      "metadata": {
        "id": "sMfm67TZbL1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESG Agent Core Functions Definition"
      ],
      "metadata": {
        "id": "FcGjvSGdjE9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ESG Agent Core Function Definitions ===\n",
        "\n",
        "# ---------------------------------------\n",
        "# ✅ Utility\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_json_from_text(text, provider=\"gpt\"):\n",
        "    text = text.strip(\"` \\n\")\n",
        "    if text.lower().startswith(\"json\"):\n",
        "        text = text[4:].strip()\n",
        "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group()\n",
        "        json_str = json_str.replace(\"+\", \"\")\n",
        "        json_str = re.sub(r\",\\s*}\", \"}\", json_str)\n",
        "        json_str = re.sub(r\",\\s*]\", \"]\", json_str)\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            return json5.loads(json_str)\n",
        "    raise ValueError(\"❌ No JSON structure found in text.\")\n",
        "\n",
        "def token_count(prompt: str, provider: str = \"openai\") -> int:\n",
        "    \"\"\"\n",
        "    Estimates token count based on provider.\n",
        "    For OpenAI: exact via tiktoken.\n",
        "    For Claude: 1 token ≈ 3.5 characters.\n",
        "    For Gemini: 1 token ≈ 0.75 words.\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        try:\n",
        "            import tiktoken\n",
        "            enc = tiktoken.encoding_for_model(\"gpt-4\")  # or use gpt-3.5-turbo if needed\n",
        "            return len(enc.encode(prompt))\n",
        "        except Exception:\n",
        "            return len(prompt.split())  # fallback\n",
        "\n",
        "    elif provider == \"claude\":\n",
        "        return int(len(prompt) / 3.5)\n",
        "\n",
        "    elif provider == \"gemini\":\n",
        "        return int(len(prompt.split()) / 0.75)\n",
        "\n",
        "    return len(prompt.split())  # fallback\n",
        "\n",
        "def get_token_usage(response, prompt, output, provider):\n",
        "    \"\"\"\n",
        "    Attempts to extract input/output token counts from model response.\n",
        "    Falls back to estimation if usage info is not present.\n",
        "\n",
        "    Args:\n",
        "        response: The raw LLM response object (if available).\n",
        "        prompt (str): The prompt text sent to the model.\n",
        "        output (str): The output text received from the model.\n",
        "        provider (str): One of: \"openai\", \"claude\", \"gemini\".\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"input_tokens\": int,\n",
        "            \"output_tokens\": int,\n",
        "            \"total_tokens\": int\n",
        "        }\n",
        "    \"\"\"\n",
        "    input_tokens = output_tokens = 0\n",
        "\n",
        "    # ✅ OpenAI: Accurate\n",
        "    if provider == \"openai\" and hasattr(response, \"usage\") and response.usage:\n",
        "        input_tokens = response.usage.prompt_tokens\n",
        "        output_tokens = response.usage.completion_tokens\n",
        "\n",
        "    # ✅ Claude: Might not have usage\n",
        "    elif provider == \"claude\" and hasattr(response, \"usage\") and response.usage:\n",
        "        input_tokens = response.usage.input_tokens\n",
        "        output_tokens = response.usage.output_tokens\n",
        "\n",
        "    # ⚠️ Gemini or fallback\n",
        "    else:\n",
        "        input_tokens = token_count(prompt, provider)\n",
        "        output_tokens = token_count(output, provider)\n",
        "\n",
        "    return {\n",
        "        \"input_tokens\": input_tokens,\n",
        "        \"output_tokens\": output_tokens,\n",
        "        \"total_tokens\": input_tokens + output_tokens\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# ✅ Prompt Builders\n",
        "# ---------------------------------------\n",
        "\n",
        "def build_metadata_prompt(disclosure, matched_text):\n",
        "    return f\"\"\"\n",
        "You are an ESG metadata extraction assistant.\n",
        "\n",
        "Your task is to extract structured ESG metadata from the matched company report excerpt.\n",
        "\n",
        "=== DISCLOSURE ===\n",
        "ID: {disclosure.get('id')}\n",
        "Title: {disclosure.get('title')}\n",
        "Description: {disclosure.get('description')}\n",
        "\n",
        "=== MATCHED TEXT ===\n",
        "{matched_text}\n",
        "\n",
        "=== OUTPUT INSTRUCTIONS ===\n",
        "- Include the full title of the disclosure.\n",
        "- Extract any **quantitative indicators** (numerical KPIs, metrics, values) found in the text.\n",
        "- Also include a **qualitative summary** that captures policies, initiatives, or narrative context.\n",
        "- If no quantitative indicators exist, return an empty list.\n",
        "- If no qualitative content exists, return an empty string.\n",
        "\n",
        "=== OUTPUT FORMAT ===\n",
        "{{\n",
        "  \"disclosure_id\": \"{disclosure.get('id')}\",\n",
        "  \"title\": \"{disclosure.get('title')}\",\n",
        "  \"qualitative_summary\": \"Brief ESG-relevant summary here.\",\n",
        "  \"quantitative_indicators\": [\n",
        "    {{\n",
        "      \"name\": \"metric_name\",\n",
        "      \"value\": 123.45,\n",
        "      \"unit\": \"tons\",\n",
        "      \"year\": 2022\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "- Do NOT return markdown, bullet points, or any explanation.\n",
        "- Return only clean JSON exactly in the above format.\n",
        "\"\"\".strip()\n",
        "\n",
        "def build_metadata_eval_prompt(disclosure, metadata):\n",
        "    return f\"\"\"\n",
        "You are an ESG compliance auditor.\n",
        "\n",
        "Evaluate whether the company’s ESG disclosure meets the requirement below.\n",
        "\n",
        "=== DISCLOSURE REQUIREMENT ===\n",
        "ID: {disclosure.get(\"id\")}\n",
        "Title: {disclosure.get(\"title\")}\n",
        "Description: {disclosure.get(\"description\")}\n",
        "\n",
        "=== COMPANY METADATA ===\n",
        "{json.dumps(metadata, indent=2)}\n",
        "\n",
        "== YOUR TASK ==\n",
        "- Assign one of the following ratings:\n",
        "  - Fully Compliant (score: 1.0)\n",
        "  - Partially Compliant (score: 0.5)\n",
        "  - Non-Compliant (score: 0.0)\n",
        "\n",
        "- Justify your decision with clear reasoning (e.g. presence/absence of KPIs, vague policies, incomplete scope).\n",
        "\n",
        "- Format your output in **valid JSON** exactly like this:\n",
        "{{\n",
        "  \"rating\": \"Fully Compliant\",\n",
        "  \"score\": 1.0,\n",
        "  \"justification\": \"Includes three KPIs with targets and a clear policy summary.\"\n",
        "}}\n",
        "\n",
        "Respond only with clean JSON. No markdown or extra explanation.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def build_fix_suggestion_prompt(disclosure_id, rating, reason, rule_text):\n",
        "    return f\"\"\"\n",
        "You are an ESG policy analyst.\n",
        "\n",
        "A disclosure was marked as '{rating}' in an ESG compliance audit.\n",
        "\n",
        "Disclosure ID: {disclosure_id}\n",
        "Rule Description:\n",
        "{rule_text.strip()}\n",
        "\n",
        "❌ Gap Identified:\n",
        "{reason.strip()}\n",
        "\n",
        "Suggest one clear improvement that could help the company align better with this rule. Return plain text only. No markdown or headers.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 👨‍💼 Admin Functions (to be run once)\n",
        "# ---------------------------------------\n",
        "\n",
        "def load_gri_mapping_with_deprecated(path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_excel(path, sheet_name=\"2. 2016 to 2021 \", header=25, engine=\"openpyxl\")\n",
        "    df = df.rename(columns={\"Unnamed: 2\": \"Disclosure_2016\", \"Unnamed: 7\": \"Disclosure_2021\"})\n",
        "    df[\"Disclosure_2016_base\"] = df[\"Disclosure_2016\"].astype(str).str.extract(r\"(\\d{3}-\\d+)\")\n",
        "    df = df.dropna(subset=[\"Disclosure_2016_base\"])\n",
        "    mapping, deprecated = {}, []\n",
        "    for _, row in df.iterrows():\n",
        "        old_id = row[\"Disclosure_2016_base\"]\n",
        "        new_id = row[\"Disclosure_2021\"]\n",
        "        mapping[old_id] = str(new_id).strip() if pd.notna(new_id) and str(new_id).upper() != \"N/A\" else None\n",
        "        if mapping[old_id] is None: deprecated.append(old_id)\n",
        "    for mid in {\"412-1\", \"412-2\", \"412-3\"}:\n",
        "        if mid not in mapping:\n",
        "            mapping[mid] = None\n",
        "            deprecated.append(mid)\n",
        "    return mapping, set(deprecated)\n",
        "\n",
        "def admin_load_gri_mapping_and_save(path):\n",
        "    mapping, deprecated = load_gri_mapping_with_deprecated(path)\n",
        "    os.makedirs(\"admin\", exist_ok=True)\n",
        "    with open(\"admin/gri_mapping.json\", \"w\") as f:\n",
        "        json.dump({\"mapping\": mapping, \"deprecated\": list(deprecated)}, f)\n",
        "    print(f\"✅ GRI mapping saved. Entries: {len(mapping)}, Deprecated: {len(deprecated)}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 👤 User Runtime Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "def load_admin_disclosures():\n",
        "    with open(\"admin/all_disclosures.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_gri_mapping():\n",
        "    with open(\"admin/gri_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return data[\"mapping\"], set(data[\"deprecated\"])\n",
        "\n",
        "def normalize_gri_id(disclosure_id, mapping):\n",
        "    base_id = re.match(r\"(\\d{3}-\\d+)\", disclosure_id.strip())\n",
        "    base_id = base_id.group(1) if base_id else disclosure_id.strip()\n",
        "    return mapping.get(base_id) or base_id\n",
        "\n",
        "# ---------------------------------------\n",
        "# 📄 ESG Report Parsing\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_text_docling(pdf_path, min_chunk_chars=300):\n",
        "    from docling.document_converter import DocumentConverter\n",
        "    from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "    from docling.document_converter import PdfFormatOption, InputFormat\n",
        "    from docling.backend.docling_parse_v2_backend import DoclingParseV2DocumentBackend\n",
        "\n",
        "    pipeline_options = PdfPipelineOptions(do_ocr=False, do_table_structure=False)\n",
        "\n",
        "    doc_converter = DocumentConverter(\n",
        "        format_options={\n",
        "            InputFormat.PDF: PdfFormatOption(\n",
        "                pipeline_options=pipeline_options,\n",
        "                backend=DoclingParseV2DocumentBackend\n",
        "            )\n",
        "        }\n",
        "    )\n",
        "\n",
        "    result = doc_converter.convert(pdf_path)\n",
        "    full_text = result.document.export_to_markdown()\n",
        "\n",
        "    buffer, chunks = \"\", []\n",
        "    for line in full_text.splitlines():\n",
        "        buffer += line.strip() + \" \"\n",
        "        if len(buffer) >= min_chunk_chars:\n",
        "            chunks.append(buffer.strip())\n",
        "            buffer = \"\"\n",
        "    if buffer:\n",
        "        chunks.append(buffer.strip())\n",
        "    return full_text, chunks\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 🧠 Vectorstore Builders\n",
        "# ---------------------------------------\n",
        "\n",
        "def build_disclosure_rule_vectorstore(disclosures, path=\"disclosure_rule_vectorstore\", force=False):\n",
        "    from langchain.vectorstores import FAISS\n",
        "    from langchain.schema import Document\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "    embedding = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "    if not os.path.exists(path) or force:\n",
        "        docs = [\n",
        "            Document(page_content=f\"{d['title']}\\n\\n{d['description']}\", metadata={\"id\": d[\"id\"]})\n",
        "            for d in disclosures if d.get(\"description\")\n",
        "        ]\n",
        "        store = FAISS.from_documents(docs, embedding)\n",
        "        store.save_local(path)\n",
        "    else:\n",
        "        store = FAISS.load_local(path, embedding, allow_dangerous_deserialization=True)\n",
        "    return store\n",
        "\n",
        "def build_runtime_state_index(results: dict, rebuild=False):\n",
        "    from langchain.vectorstores import FAISS\n",
        "    from langchain.schema import Document\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    import json, os\n",
        "\n",
        "    FAISS_INDEX_PATH = \"faiss_global_index\"\n",
        "    embedding_model = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "\n",
        "    if os.path.exists(FAISS_INDEX_PATH) and not rebuild:\n",
        "        print(\"✅ Loading saved FAISS index...\")\n",
        "        return FAISS.load_local(FAISS_INDEX_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "    docs = []\n",
        "    section_counts = {}\n",
        "\n",
        "    for section, content in results.items():\n",
        "        section_doc_count = 0\n",
        "\n",
        "        if isinstance(content, list):  # e.g., matching\n",
        "            for item in content:\n",
        "                if not isinstance(item, dict):\n",
        "                    continue\n",
        "                metadata = {\n",
        "                    \"section\": section,\n",
        "                    \"id\": item.get(\"id\") or item.get(\"disclosure_id\") or item.get(\"original_id\"),\n",
        "                    \"title\": item.get(\"title\"),\n",
        "                    \"status\": item.get(\"status\"),\n",
        "                    \"rating\": item.get(\"rating\"),\n",
        "                    \"reason\": item.get(\"reason\"),\n",
        "                    \"source\": item.get(\"source\"),\n",
        "                }\n",
        "                page_content = json.dumps(item, indent=2, ensure_ascii=False)\n",
        "                docs.append(Document(page_content=page_content, metadata=metadata))\n",
        "                section_doc_count += 1\n",
        "\n",
        "        elif isinstance(content, dict):  # e.g., metadata, evaluation, fixes (keyed by model)\n",
        "            for model, items in content.items():\n",
        "                if isinstance(items, dict):\n",
        "                    items = list(items.values())\n",
        "                elif not isinstance(items, list):\n",
        "                    continue\n",
        "\n",
        "                for item in items:\n",
        "                    if not isinstance(item, dict):\n",
        "                        continue\n",
        "                    metadata = {\n",
        "                        \"section\": section,\n",
        "                        \"model\": model,\n",
        "                        \"id\": item.get(\"id\") or item.get(\"disclosure_id\"),\n",
        "                        \"rating\": item.get(\"rating\"),\n",
        "                        \"reason\": item.get(\"reason\"),\n",
        "                    }\n",
        "                    page_content = json.dumps(item, indent=2, ensure_ascii=False)\n",
        "                    docs.append(Document(page_content=page_content, metadata=metadata))\n",
        "                    section_doc_count += 1\n",
        "\n",
        "        section_counts[section] = section_doc_count\n",
        "\n",
        "\n",
        "def build_esg_report_index(text_chunks, rebuild=False):\n",
        "    from langchain.vectorstores import FAISS\n",
        "    from langchain.schema import Document\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    import os\n",
        "\n",
        "    FAISS_INDEX_PATH = \"faiss_content_index\"\n",
        "    embedding = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "\n",
        "    if os.path.exists(FAISS_INDEX_PATH) and not rebuild:\n",
        "        print(\"✅ Loading saved ESG content index...\")\n",
        "        return FAISS.load_local(FAISS_INDEX_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "    print(\"📄 Building ESG content index from chunks...\")\n",
        "\n",
        "    # Create Document objects with chunk_index metadata\n",
        "    docs = [\n",
        "        Document(page_content=chunk, metadata={\"chunk_index\": i})\n",
        "        for i, chunk in enumerate(text_chunks)\n",
        "    ]\n",
        "\n",
        "    index = FAISS.from_documents(docs, embedding)\n",
        "    index.save_local(FAISS_INDEX_PATH)\n",
        "    return index\n",
        "\n",
        "\n",
        "def determine_metadata_source():\n",
        "    from pathlib import Path\n",
        "\n",
        "    llm_all_exist = all(Path(f\"llm_matching_outputs/llm_matched_{model}.json\").exists() for model in SELECTED_MODELS)\n",
        "    traditional_exists = Path(\"matched_traditional_best.json\").exists()\n",
        "\n",
        "    if llm_all_exist and traditional_exists:\n",
        "        print(\"🧠 Both LLM and Traditional match files are available.\")\n",
        "        print(\"⚠️ Cannot prompt for input interactively in this environment.\")\n",
        "        print(\"👉 Defaulting to LLM matches. Set explicitly if needed.\")\n",
        "        return \"llm\"\n",
        "    elif llm_all_exist:\n",
        "        return \"llm\"\n",
        "    elif traditional_exists:\n",
        "        return \"traditional\"\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 🔍 Disclosure Matching\n",
        "# ---------------------------------------\n",
        "\n",
        "def safe_description(d):\n",
        "    return d[\"description\"].strip() if d.get(\"description\", \"\").strip() else f\"This disclosure requires information about: {d['title']}\"\n",
        "\n",
        "#This function gets called by matche_dicslosures function if method =\"llm\"\n",
        "    return d[\"description\"].strip() if d.get(\"description\", \"\").strip() else f\"This disclosure requires information about: {d['title']}\"\n",
        "def match_disclosures_with_llm(disclosures, esg_vectorstore, model_label, provider_config, top_k=10, return_raw=False):\n",
        "    import json\n",
        "    from tqdm import tqdm\n",
        "    import os\n",
        "    from openai import OpenAI\n",
        "    import anthropic\n",
        "    import google.generativeai as genai\n",
        "    from langchain_core.vectorstores import VectorStoreRetriever\n",
        "    from time import time\n",
        "\n",
        "    model_info = provider_config[model_label]\n",
        "    model = model_info[\"model\"]\n",
        "    provider = model_info[\"provider\"]\n",
        "\n",
        "    if provider == \"openai\":\n",
        "        client = OpenAI()\n",
        "    elif provider == \"claude\":\n",
        "        client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "    elif provider == \"gemini\":\n",
        "        genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "        client = genai.GenerativeModel(model)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported provider: {provider}\")\n",
        "\n",
        "    retriever: VectorStoreRetriever = esg_vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "    results = []\n",
        "    raw_logs = []\n",
        "\n",
        "    for d in tqdm(disclosures, desc=f\"🔍 Matching with {model_label}\"):\n",
        "        query = f\"{d.get('title', '')}\\n\\n{safe_description(d)}\"\n",
        "        docs_with_scores = esg_vectorstore.similarity_search_with_score(query, k=top_k)\n",
        "\n",
        "        prompt_chunks = []\n",
        "        chunk_lookup = {}\n",
        "        for i, (doc, score) in enumerate(docs_with_scores):\n",
        "            chunk_id = f\"Chunk {i+1}\"\n",
        "            chunk_lookup[chunk_id] = doc.page_content\n",
        "            prompt_chunks.append(f\"[{chunk_id}]:\\n{doc.page_content.strip()}\")\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are an expert sustainability auditor. Your task is to determine which ESG report chunks are relevant to the disclosure requirement below.\n",
        "\n",
        "Use your understanding of ESG standards to identify which chunks support or demonstrate disclosure of the required information. Prioritize factual alignment, not keyword overlap.\n",
        "\n",
        "=== DISCLOSURE REQUIREMENT ===\n",
        "ID: {d.get(\"id\")}\n",
        "Title: {d.get(\"title\")}\n",
        "Details:\n",
        "{d.get(\"description\")}\n",
        "\n",
        "=== ESG REPORT EXCERPTS ===\n",
        "Each chunk is labeled. Review them carefully:\n",
        "{chr(10).join(prompt_chunks)}\n",
        "\n",
        "=== OUTPUT FORMAT ===\n",
        "Respond with a valid JSON object in this format:\n",
        "{{\n",
        "  \"matched_chunks\": [\"Chunk 2\", \"Chunk 4\"],\n",
        "  \"justification\": \"Chunk 2 includes packaging KPIs; Chunk 4 discusses recycling systems relevant to the disclosure.\"\n",
        "}}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Match only chunks that directly help fulfill or evidence the disclosure.\n",
        "- Do not select vague, partial, or unrelated chunks.\n",
        "- If no chunk is relevant, return an empty list.\n",
        "- Respond with JSON only. Do not add explanation, markdown, or extra commentary.\n",
        "\"\"\".strip()\n",
        "\n",
        "        raw_output = \"\"\n",
        "        latency = None\n",
        "        error = None\n",
        "        usage = {}\n",
        "\n",
        "        try:\n",
        "            start_time = time()\n",
        "\n",
        "            if provider == \"openai\":\n",
        "                response = client.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.2,\n",
        "                    max_tokens=1000\n",
        "                )\n",
        "                raw_output = response.choices[0].message.content.strip()\n",
        "\n",
        "            elif provider == \"claude\":\n",
        "                response = client.messages.create(\n",
        "                    model=model,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.2,\n",
        "                    max_tokens=1000\n",
        "                )\n",
        "                raw_output = response.content[0].text.strip()\n",
        "\n",
        "            elif provider == \"gemini\":\n",
        "                response = client.generate_content(prompt)\n",
        "                raw_output = response.text.strip()\n",
        "\n",
        "            latency = round(time() - start_time, 2)\n",
        "            usage = get_token_usage(response, prompt, raw_output, provider)\n",
        "\n",
        "            raw_output = raw_output.strip(\"` \\n\")\n",
        "            if raw_output.lower().startswith(\"json\"):\n",
        "                raw_output = raw_output[4:].strip()\n",
        "\n",
        "            parsed = json.loads(raw_output)\n",
        "            matched_chunks = [chunk_lookup[cid] for cid in parsed.get(\"matched_chunks\", []) if cid in chunk_lookup]\n",
        "\n",
        "            results.append({\n",
        "                \"id\": d.get(\"id\"),\n",
        "                \"title\": d.get(\"title\", \"\"),\n",
        "                \"source\": d.get(\"source\", \"GRI_Topic\"),\n",
        "                \"matched_chunks\": matched_chunks,\n",
        "                \"status\": \"matched\" if matched_chunks else \"no_match\"\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            error = str(e)\n",
        "            results.append({\n",
        "                \"id\": d.get(\"id\"),\n",
        "                \"title\": d.get(\"title\", \"\"),\n",
        "                \"source\": d.get(\"source\", \"GRI_Topic\"),\n",
        "                \"matched_chunks\": [],\n",
        "                \"status\": \"no_match\",\n",
        "                \"error\": error\n",
        "            })\n",
        "\n",
        "        raw_logs.append({\n",
        "            \"disclosure_id\": d.get(\"id\"),\n",
        "            \"title\": d.get(\"title\"),\n",
        "            \"provider\": provider,\n",
        "            \"model\": model,\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": raw_output,\n",
        "            \"latency_sec\": latency,\n",
        "            **usage,\n",
        "            \"error\": error\n",
        "        })\n",
        "\n",
        "    return (results, raw_logs) if return_raw else results\n",
        "\n",
        "\n",
        "def match_disclosures(disclosures, esg_vectorstore, top_k=10, similarity_threshold=1.2, method=\"llm\", provider_config=None, verbose=False):\n",
        "    from tqdm import tqdm\n",
        "    from langchain_core.vectorstores import VectorStoreRetriever\n",
        "    import json\n",
        "    import os\n",
        "    from copy import deepcopy\n",
        "\n",
        "    global GRI_ID_MAP\n",
        "    os.makedirs(\"llm_matching_outputs\", exist_ok=True)\n",
        "\n",
        "    if method == \"traditional\":\n",
        "        results = []\n",
        "        retriever: VectorStoreRetriever = esg_vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "        for d in tqdm(disclosures, desc=\"Matching (Traditional)\"):\n",
        "            raw_id = d.get(\"id\", \"UNKNOWN\")\n",
        "            id_ = normalize_gri_id(raw_id, GRI_ID_MAP)\n",
        "            query = f\"{d.get('title', '')}\\n\\n{safe_description(d)}\"\n",
        "            docs_with_scores = esg_vectorstore.similarity_search_with_score(query, k=top_k)\n",
        "\n",
        "            matched_chunks = []\n",
        "            matched_scores = []\n",
        "\n",
        "            for doc, score in docs_with_scores:\n",
        "                if score <= similarity_threshold:\n",
        "                    matched_chunks.append(doc.page_content)\n",
        "                    matched_scores.append(float(score))\n",
        "\n",
        "            results.append({\n",
        "                \"original_id\": raw_id,\n",
        "                \"id\": id_,\n",
        "                \"title\": d.get(\"title\", \"\"),\n",
        "                \"description\": d.get(\"description\", \"\"),\n",
        "                \"source\": d.get(\"source\", \"\"),\n",
        "                \"matched_chunks\": matched_chunks,\n",
        "                \"similarity_scores\": matched_scores,\n",
        "                \"status\": \"matched\" if matched_chunks else \"no_match\"\n",
        "            })\n",
        "\n",
        "        with open(\"matched_traditional_best.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        with open(\"matched_only.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return results\n",
        "\n",
        "    elif method == \"llm\":\n",
        "        assert provider_config is not None, \"❌ LLM mode requires provider_config\"\n",
        "\n",
        "        all_results = {}\n",
        "        all_raw_logs = {}\n",
        "\n",
        "        for model_name, config in provider_config.items():\n",
        "            print(f\"\\n🤖 Matching with {model_name.upper()}\")\n",
        "\n",
        "            # Capture both results and raw logs\n",
        "            results, raw_logs = match_disclosures_with_llm(\n",
        "                disclosures=deepcopy(disclosures),\n",
        "                esg_vectorstore=esg_vectorstore,\n",
        "                model_label=model_name,\n",
        "                provider_config={model_name: config},\n",
        "                top_k=top_k,\n",
        "                return_raw=True  # Ensure your inner function supports this\n",
        "            )\n",
        "            all_results[model_name] = results\n",
        "            all_raw_logs[model_name] = raw_logs\n",
        "\n",
        "            with open(f\"llm_matching_outputs/llm_matched_{model_name}.json\", \"w\") as f:\n",
        "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            with open(f\"llm_matching_outputs/llm_raw_matching_{model_name}.json\", \"w\") as f:\n",
        "                json.dump(raw_logs, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"❌ Unknown matching method. Use 'traditional' or 'llm'.\")\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# 💾 Save Helpers\n",
        "# ---------------------------------------\n",
        "\n",
        "def convert_to_serializable(obj):\n",
        "    import numpy as np\n",
        "\n",
        "    if isinstance(obj, (float, int, str, bool, type(None))):\n",
        "        return obj\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, list):\n",
        "        return [convert_to_serializable(i) for i in obj]\n",
        "    if isinstance(obj, (np.integer, np.floating)):\n",
        "        return obj.item()\n",
        "    if hasattr(obj, \"tolist\"):  # e.g., np.ndarray\n",
        "        return obj.tolist()\n",
        "    return str(obj)\n",
        "\n",
        "\n",
        "\n",
        "def save_matches(matches, filename=\"matched_disclosures_full.json\"):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(convert_to_serializable(matches), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "def save_matched_only(results, output_path=\"matched_only.json\"):\n",
        "    matched = [r for r in results if r.get(\"status\") == \"matched\"]\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(convert_to_serializable(matched), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# ---------------------------------------\n",
        "# 🤖 Agentic AI Runtime: Metadata & Fixes\n",
        "# ---------------------------------------\n",
        "def run_metadata_extraction(all_disclosures, providers, selected_models, extraction_match_source: str = \"llm\"):\n",
        "    import anthropic\n",
        "    import google.generativeai as genai\n",
        "    from openai import OpenAI\n",
        "    import os, json, time\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    os.makedirs(\"metadata_outputs\", exist_ok=True)\n",
        "\n",
        "    for label in selected_models:\n",
        "        opts = providers[label]\n",
        "        results = {}\n",
        "        raw_log = {}\n",
        "\n",
        "        # Initialize model client\n",
        "        if opts[\"provider\"] == \"openai\":\n",
        "            client = OpenAI()\n",
        "        elif opts[\"provider\"] == \"claude\":\n",
        "            client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "        elif opts[\"provider\"] == \"gemini\":\n",
        "            genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "            model = genai.GenerativeModel(opts[\"model\"])\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported provider\")\n",
        "\n",
        "        for d in tqdm(all_disclosures, desc=f\"🔎 {label} Metadata Extraction Processing\"):\n",
        "            if d.get(\"status\") != \"matched\" or not d.get(\"matched_chunks\"):\n",
        "                continue\n",
        "\n",
        "            context = \"\\n\\n---\\n\\n\".join(d[\"matched_chunks\"])\n",
        "            prompt = build_metadata_prompt(d, context)\n",
        "            disclosure_id = d[\"id\"]\n",
        "\n",
        "            start_time = time.time()\n",
        "            output = \"\"\n",
        "            res = None\n",
        "\n",
        "            try:\n",
        "                if opts[\"provider\"] == \"openai\":\n",
        "                    res = client.chat.completions.create(\n",
        "                        model=opts[\"model\"],\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        temperature=0.2,\n",
        "                        max_tokens=2000\n",
        "                    )\n",
        "                    output = res.choices[0].message.content.strip()\n",
        "\n",
        "                elif opts[\"provider\"] == \"claude\":\n",
        "                    res = client.messages.create(\n",
        "                        model=opts[\"model\"],\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        temperature=0.2,\n",
        "                        max_tokens=1024\n",
        "                    )\n",
        "                    output = res.content[0].text.strip()\n",
        "\n",
        "                elif opts[\"provider\"] == \"gemini\":\n",
        "                    res = model.generate_content(prompt)\n",
        "                    output = res.text.strip(\"`\\n \")\n",
        "\n",
        "                latency = round(time.time() - start_time, 2)\n",
        "                usage = get_token_usage(res, prompt, output, opts[\"provider\"])\n",
        "\n",
        "                parsed = extract_json_from_text(output, provider=opts[\"provider\"])\n",
        "                parsed[\"disclosure_id\"] = disclosure_id\n",
        "                results[disclosure_id] = parsed\n",
        "\n",
        "                raw_log[disclosure_id] = {\n",
        "                    \"prompt\": prompt,\n",
        "                    \"output\": output,\n",
        "                    **usage,\n",
        "                    \"latency_sec\": latency\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                results[disclosure_id] = {\"error\": str(e)}\n",
        "                raw_log[disclosure_id] = {\n",
        "                    \"prompt\": prompt,\n",
        "                    \"output\": f\"[LLM Error: {e}]\"\n",
        "                }\n",
        "\n",
        "        method_tag = extraction_match_source.lower()\n",
        "        with open(f\"metadata_outputs/esg_metadata_{label.lower()}_from_{method_tag}.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        with open(f\"metadata_outputs/esg_metadata_{label.lower()}_raw_responses_{method_tag}.json\", \"w\") as f:\n",
        "            json.dump(raw_log, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def run_llm_metadata_evaluation(disclosures, metadata_by_model, model_label, model_config, output_prefix=None):\n",
        "    import anthropic\n",
        "    import google.generativeai as genai\n",
        "    from openai import OpenAI\n",
        "    import os\n",
        "    import json\n",
        "    import time\n",
        "\n",
        "    results = {}\n",
        "    raw_log = {}\n",
        "\n",
        "    if model_config[\"provider\"] == \"openai\":\n",
        "        client = OpenAI()\n",
        "    elif model_config[\"provider\"] == \"claude\":\n",
        "        client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "    elif model_config[\"provider\"] == \"gemini\":\n",
        "        genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "        client = genai.GenerativeModel(model_config[\"model\"])\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported provider\")\n",
        "\n",
        "    for d in tqdm(disclosures, desc=f\"Evaluating {model_label}\"):\n",
        "        disclosure_id = d[\"id\"]\n",
        "        metadata = metadata_by_model.get(disclosure_id)\n",
        "\n",
        "        if not metadata or \"error\" in metadata:\n",
        "            results[disclosure_id] = {\n",
        "                \"disclosure_id\": disclosure_id,\n",
        "                \"rating\": \"Non-Compliant\",\n",
        "                \"score\": 0.0,\n",
        "                \"justification\": \"Metadata not available or extraction failed.\"\n",
        "            }\n",
        "            raw_log[disclosure_id] = \"[Missing metadata]\"\n",
        "            continue\n",
        "\n",
        "        prompt = build_metadata_eval_prompt(d, metadata)\n",
        "        start_time = time.time()\n",
        "        output = \"\"\n",
        "        res = None\n",
        "\n",
        "        try:\n",
        "            if model_config[\"provider\"] == \"openai\":\n",
        "                res = client.chat.completions.create(\n",
        "                    model=model_config[\"model\"],\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.2,\n",
        "                    max_tokens=500\n",
        "                )\n",
        "                output = res.choices[0].message.content.strip()\n",
        "\n",
        "            elif model_config[\"provider\"] == \"claude\":\n",
        "                res = client.messages.create(\n",
        "                    model=model_config[\"model\"],\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.2,\n",
        "                    max_tokens=500\n",
        "                )\n",
        "                output = res.content[0].text.strip()\n",
        "\n",
        "            elif model_config[\"provider\"] == \"gemini\":\n",
        "                res = client.generate_content(prompt)\n",
        "                output = res.text.strip(\"`\\n \")\n",
        "                if output.lower().startswith(\"json\"):\n",
        "                    output = output[output.find(\"{\"):].strip()\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported provider\")\n",
        "\n",
        "            latency = round(time.time() - start_time, 2)\n",
        "            usage = get_token_usage(res, prompt, output, model_config[\"provider\"])\n",
        "\n",
        "            parsed = json.loads(output)\n",
        "            parsed[\"disclosure_id\"] = disclosure_id\n",
        "\n",
        "            if \"rating\" not in parsed or \"score\" not in parsed or \"justification\" not in parsed:\n",
        "                raise ValueError(\"Incomplete fields in LLM response\")\n",
        "\n",
        "            results[disclosure_id] = parsed\n",
        "            raw_log[disclosure_id] = {\n",
        "                \"prompt\": prompt,\n",
        "                \"output\": output,\n",
        "                \"latency_sec\": latency,\n",
        "                **usage\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            latency = round(time.time() - start_time, 2)\n",
        "            results[disclosure_id] = {\n",
        "                \"disclosure_id\": disclosure_id,\n",
        "                \"rating\": \"Non-Compliant\",\n",
        "                \"score\": 0.0,\n",
        "                \"justification\": f\"[LLM Error: {e}]\"\n",
        "            }\n",
        "            raw_log[disclosure_id] = {\n",
        "                \"prompt\": prompt,\n",
        "                \"output\": output if 'output' in locals() else \"[No output]\",\n",
        "                \"latency_sec\": latency,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    os.makedirs(\"metadata_outputs\", exist_ok=True)\n",
        "    output_prefix = output_prefix or f\"metadata_outputs/metadata_eval_{model_label}\"\n",
        "\n",
        "    with open(f\"{output_prefix}.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    with open(f\"{output_prefix}_raw.json\", \"w\") as f:\n",
        "        json.dump(raw_log, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "#Note: Consider removing the vector store and retrieving the disclosure directly\n",
        "def generate_esg_fixes_with_retrieval_multi(compliance_report, rule_vectorstore, model_configs, selected_models, return_raw=False, fixes_match_source=None):\n",
        "    import anthropic\n",
        "    import google.generativeai as genai\n",
        "    from openai import OpenAI\n",
        "    import time\n",
        "    import os, json\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    os.makedirs(\"fix_outputs\", exist_ok=True)\n",
        "    all_raw_logs = []\n",
        "\n",
        "    for model in selected_models:\n",
        "        config = model_configs[model]\n",
        "\n",
        "        if config[\"provider\"] == \"openai\":\n",
        "            client = OpenAI()\n",
        "        elif config[\"provider\"] == \"claude\":\n",
        "            client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
        "        elif config[\"provider\"] == \"gemini\":\n",
        "            genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "            client = genai.GenerativeModel(config[\"model\"])\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported provider\")\n",
        "\n",
        "        results = []\n",
        "        raw_logs = []\n",
        "\n",
        "        for item in tqdm(compliance_report, desc=f\"{model.upper()} Fixes\"):\n",
        "            if item.get(\"rating\") not in {\"Non-Compliant\", \"Partially Compliant\"}:\n",
        "                continue\n",
        "\n",
        "            disclosure_id = item.get(\"disclosure_id\")\n",
        "            reason = item.get(\"reason\", \"No reason provided\")\n",
        "            disclosure_title = item.get(\"title\", f\"Disclosure {disclosure_id}\")\n",
        "\n",
        "            retrieved = rule_vectorstore.similarity_search(disclosure_id, k=1)\n",
        "            rule_text = retrieved[0].page_content if retrieved else \"[No rule found]\"\n",
        "            prompt = build_fix_suggestion_prompt(disclosure_id, item[\"rating\"], reason, rule_text)\n",
        "\n",
        "            start_time = time.time()\n",
        "            suggestion = \"[No output]\"\n",
        "            res = None\n",
        "\n",
        "            try:\n",
        "                if config[\"provider\"] == \"openai\":\n",
        "                    res = client.chat.completions.create(\n",
        "                        model=config[\"model\"],\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        temperature=0.2,\n",
        "                        max_tokens=300\n",
        "                    )\n",
        "                    suggestion = res.choices[0].message.content.strip()\n",
        "\n",
        "                elif config[\"provider\"] == \"claude\":\n",
        "                    res = client.messages.create(\n",
        "                        model=config[\"model\"],\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        temperature=0.2,\n",
        "                        max_tokens=300\n",
        "                    )\n",
        "                    suggestion = res.content[0].text.strip()\n",
        "\n",
        "                elif config[\"provider\"] == \"gemini\":\n",
        "                    res = client.generate_content(prompt)\n",
        "                    suggestion = res.text.strip(\"`\\n \")\n",
        "\n",
        "                latency = round(time.time() - start_time, 2)\n",
        "                usage = get_token_usage(res, prompt, suggestion, config[\"provider\"])\n",
        "\n",
        "            except Exception as e:\n",
        "                suggestion = f\"[ERROR: {e}]\"\n",
        "                latency = round(time.time() - start_time, 2)\n",
        "                usage = {\"input_tokens\": None, \"output_tokens\": None}\n",
        "\n",
        "            results.append({\n",
        "                \"disclosure_id\": disclosure_id,\n",
        "                \"title\": disclosure_title,\n",
        "                \"reason\": reason,\n",
        "                \"rating\": item[\"rating\"],\n",
        "                \"suggested_fix\": suggestion\n",
        "            })\n",
        "\n",
        "            raw_logs.append((disclosure_id, {\n",
        "                \"prompt\": prompt,\n",
        "                \"output\": suggestion,\n",
        "                \"latency_sec\": latency,\n",
        "                **usage,\n",
        "                \"rating\": item[\"rating\"]\n",
        "            }))\n",
        "\n",
        "        suffix = f\"_from_{fixes_match_source}\" if fixes_match_source else \"\"\n",
        "\n",
        "        with open(f\"fix_outputs/esg_fixes_{model}{suffix}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        raw_log_dict = {k: v for k, v in raw_logs}\n",
        "        with open(f\"fix_outputs/esg_fixes_{model}{suffix}_raw.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(raw_log_dict, f, indent=2)\n",
        "\n",
        "        all_raw_logs.extend(raw_logs)\n",
        "\n",
        "    if return_raw:\n",
        "        return all_raw_logs\n"
      ],
      "metadata": {
        "id": "_GPUb9zAjGsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def print_tree(start_path='.', indent=''):\n",
        "    for item in os.listdir(start_path):\n",
        "        path = os.path.join(start_path, item)\n",
        "        print(indent + '|-- ' + item)\n",
        "        if os.path.isdir(path):\n",
        "            print_tree(path, indent + '    ')\n",
        "\n",
        "print_tree()\n"
      ],
      "metadata": {
        "id": "g6jvEJQHmTY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Admin Side Uploads"
      ],
      "metadata": {
        "id": "F3es7xvijJMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the mapping file\n",
        "uploaded = files.upload()\n",
        "mapping_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Run this admin utility\n",
        "admin_load_gri_mapping_and_save(mapping_path)\n"
      ],
      "metadata": {
        "id": "Nt943_YojHvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_and_extract_disclosure_zips():\n",
        "    \"\"\"\n",
        "    Allow multiple ZIP uploads for GRI and SASB JSONs.\n",
        "    Returns two lists: all_gri_files, all_sasb_files\n",
        "    \"\"\"\n",
        "\n",
        "    all_gri_files = []\n",
        "    all_sasb_files = []\n",
        "\n",
        "    print(\"📁 Upload GRI and SASB ZIPs one at a time. Type 'no' to stop.\\n\")\n",
        "\n",
        "    while True:\n",
        "        print(\"🔼 Upload a ZIP file:\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"⚠️ No file uploaded.\")\n",
        "            break\n",
        "\n",
        "        zip_name = list(uploaded.keys())[0]\n",
        "        extract_dir = f\"/content/tmp_extracted_{zip_name.replace('.zip','')}\"\n",
        "        os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "        with zipfile.ZipFile(zip_name, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "        # Classify files based on filename pattern\n",
        "        for root, _, files_in_dir in os.walk(extract_dir):\n",
        "            for f in files_in_dir:\n",
        "                if not f.endswith(\".json\"):\n",
        "                    continue\n",
        "                full_path = os.path.join(root, f)\n",
        "\n",
        "                if f.startswith(\"SASB_\") or \"sasb\" in f.lower():\n",
        "                    all_sasb_files.append(full_path)\n",
        "                elif f.startswith(\"GRI_\") or \"gri\" in f.lower():\n",
        "                    all_gri_files.append(full_path)\n",
        "\n",
        "        print(f\"✅ Extracted from {zip_name}: GRI: {len(all_gri_files)}, SASB: {len(all_sasb_files)}\")\n",
        "\n",
        "        cont = input(\"➕ Upload another ZIP? (yes/no): \").strip().lower()\n",
        "        if cont in [\"no\", \"n\"]:\n",
        "            break\n",
        "\n",
        "    print(f\"\\n📂 Total GRI files: {len(all_gri_files)} | SASB files: {len(all_sasb_files)}\")\n",
        "    return all_gri_files, all_sasb_files\n",
        "\n",
        "\n",
        "def load_disclosures(gri_json_files, sasb_json_files):\n",
        "    DISCLOSURES = []\n",
        "\n",
        "    # === GRI files ===\n",
        "    for path in gri_json_files:\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load GRI JSON: {path} — {e}\")\n",
        "            continue\n",
        "\n",
        "        fname = os.path.basename(path)\n",
        "\n",
        "        # GRI 3 (structure → disclosures)\n",
        "        if \"GRI_3\" in fname or \"GRI 3\" in fname:\n",
        "            if \"structure\" in data and \"disclosures\" in data[\"structure\"]:\n",
        "                for entry in data[\"structure\"][\"disclosures\"]:\n",
        "                    DISCLOSURES.append({\n",
        "                        \"id\": f\"3-{entry['id'].split('-')[-1]}\",\n",
        "                        \"title\": entry[\"title\"],\n",
        "                        \"description\": \"\\n\".join(entry.get(\"requirements\", [])),\n",
        "                        \"source\": \"GRI_3\"\n",
        "                    })\n",
        "\n",
        "        # GRI 2 (standards list)\n",
        "        if \"GRI_2\" in fname or \"GRI 2\" in fname:\n",
        "            for item in data.get(\"standards\", []):\n",
        "                DISCLOSURES.append({\n",
        "                    \"id\": item[\"code\"],\n",
        "                    \"title\": item[\"title\"],\n",
        "                    \"description\": item.get(\"description\", \"\"),\n",
        "                    \"source\": \"GRI_2\"\n",
        "                })\n",
        "\n",
        "        # GRI Topic Standards (main enhancement here)\n",
        "        if \"GRI\" in fname:\n",
        "            disclosures_found = False\n",
        "            standard = data.get(\"standard\", {})\n",
        "            sources = [\n",
        "                data.get(\"topic_disclosures\", []),\n",
        "                data.get(\"disclosures\", [])\n",
        "            ]\n",
        "\n",
        "            if isinstance(standard, dict):\n",
        "                sources += [\n",
        "                    standard.get(\"topic_disclosures\", []),\n",
        "                    standard.get(\"disclosures\", []),\n",
        "                    standard.get(\"management_disclosure\", {}).get(\"disclosures\", [])\n",
        "                ]\n",
        "\n",
        "            for source in sources:\n",
        "                for item in source:\n",
        "                    raw_id = item.get(\"id\", \"UNKNOWN\")\n",
        "\n",
        "                    # 🔧 Enhanced description construction\n",
        "                    description_parts = []\n",
        "\n",
        "                    if \"description\" in item and item[\"description\"].strip():\n",
        "                        description_parts.append(item[\"description\"].strip())\n",
        "\n",
        "                    if \"narratives\" in item and isinstance(item[\"narratives\"], list):\n",
        "                        description_parts.append(\" \".join(item[\"narratives\"]))\n",
        "\n",
        "                    if \"requirements\" in item and isinstance(item[\"requirements\"], list):\n",
        "                        description_parts.append(\"Requirements: \" + \" \".join(item[\"requirements\"]))\n",
        "\n",
        "                    if \"guidance\" in item and isinstance(item[\"guidance\"], list):\n",
        "                        description_parts.append(\"Guidance: \" + \" \".join(item[\"guidance\"]))\n",
        "\n",
        "                    combined_description = \"\\n\\n\".join(description_parts)\n",
        "\n",
        "                    DISCLOSURES.append({\n",
        "                        \"original_id\": raw_id,\n",
        "                        \"id\": raw_id,\n",
        "                        \"title\": item.get(\"title\", \"\"),\n",
        "                        \"description\": combined_description,\n",
        "                        \"source\": \"GRI_Topic\"\n",
        "                    })\n",
        "\n",
        "                    disclosures_found = True\n",
        "\n",
        "            if not disclosures_found:\n",
        "                print(f\"⚠️ Could not extract disclosures from {fname}\")\n",
        "\n",
        "    # === SASB files ===\n",
        "    for path in sasb_json_files:\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load SASB JSON: {path} — {e}\")\n",
        "            continue\n",
        "\n",
        "        sector = data.get(\"sector\", \"Unknown Sector\")\n",
        "\n",
        "        for topic in data.get(\"disclosure_topics\", []):\n",
        "            topic_name = topic.get(\"topic\", \"Untitled Topic\")\n",
        "            for metric in topic.get(\"metrics\", []):\n",
        "                raw_id = metric.get(\"id\", \"UNKNOWN\")\n",
        "                DISCLOSURES.append({\n",
        "                    \"original_id\": raw_id,\n",
        "                    \"id\": raw_id,\n",
        "                    \"title\": f\"{topic_name} – {raw_id}\",\n",
        "                    \"description\": metric.get(\"description\", \"\"),\n",
        "                    \"source\": f\"SASB ({sector})\"\n",
        "                })\n",
        "\n",
        "        for metric in data.get(\"activity_metrics\", []):\n",
        "            raw_id = metric.get(\"id\", \"UNKNOWN\")\n",
        "            DISCLOSURES.append({\n",
        "                \"original_id\": raw_id,\n",
        "                \"id\": raw_id,\n",
        "                \"title\": f\"Activity Metric – {raw_id}\",\n",
        "                \"description\": metric.get(\"description\", \"\"),\n",
        "                \"source\": f\"SASB ({sector})\"\n",
        "            })\n",
        "\n",
        "    print(f\"✅ Loaded {len(DISCLOSURES)} total disclosures.\")\n",
        "    return DISCLOSURES\n",
        "\n",
        "\n",
        "def admin_prepare_and_save_disclosures_full():\n",
        "    \"\"\"\n",
        "    Admin step: Upload and load *all* GRI and SASB disclosures (no filtering).\n",
        "    Saves to admin/all_disclosures.json for user runtime.\n",
        "    \"\"\"\n",
        "    print(\"📦 Upload GRI and SASB ZIP files now...\")\n",
        "    gri_files, sasb_files = upload_and_extract_disclosure_zips()\n",
        "\n",
        "    # ✅ Load everything, no sector filter\n",
        "    DISCLOSURES = load_disclosures(gri_files, sasb_files)\n",
        "\n",
        "    os.makedirs(\"admin\", exist_ok=True)\n",
        "    with open(\"admin/all_disclosures.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(DISCLOSURES, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✅ Saved {len(DISCLOSURES)} total disclosures from GRI and SASB to admin/all_disclosures.json\")\n"
      ],
      "metadata": {
        "id": "xIzBsCfXjKwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "admin_prepare_and_save_disclosures_full()"
      ],
      "metadata": {
        "id": "XESizIlUjL7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh admin/"
      ],
      "metadata": {
        "id": "T5wLKSK9jL-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agentic AI Tools Definition"
      ],
      "metadata": {
        "id": "Bsh8zo1zuDSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool\n",
        "from pydantic import BaseModel, Field\n",
        "from datetime import datetime\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "import functools\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "# === Runtime logging ===\n",
        "TOOL_LATENCY_LOG = {}\n",
        "LLM_COST_LOG = {}\n",
        "\n",
        "# === Cost rates (per million tokens) ===\n",
        "LLM_PRICING = {\n",
        "    \"gpt\": {\"input_price\": 0.10 / 1e6, \"output_price\": 0.40 / 1e6},\n",
        "    \"claude\": {\"input_price\": 0.80 / 1e6, \"output_price\": 4.00 / 1e6},\n",
        "    \"gemini\": {\"input_price\": 0.10 / 1e6, \"output_price\": 0.40 / 1e6},\n",
        "}\n",
        "\n",
        "\n",
        "def track_latency(tool_name):\n",
        "    def wrapper(func):\n",
        "        @functools.wraps(func)  # ✅ preserves docstring, name, etc.\n",
        "        def timed(*args, **kwargs):\n",
        "            start = time.time()\n",
        "            result = func(*args, **kwargs)\n",
        "            TOOL_LATENCY_LOG[tool_name] = round(time.time() - start, 2)\n",
        "            return result\n",
        "        return timed\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@tool(return_direct=True)\n",
        "@track_latency(\"tool_upload_esg_pdf\")\n",
        "def tool_upload_esg_pdf() -> str:\n",
        "    \"\"\"Upload and extract an ESG report PDF using Docling pipeline. Automatically resets pipeline state.\"\"\"\n",
        "    import os\n",
        "    import shutil\n",
        "    from pathlib import Path\n",
        "    from google.colab import files  # if in Colab, else adjust\n",
        "\n",
        "    # 🔄 Hard reset of pipeline globals\n",
        "    global full_text, ESG_CHUNKS, ESG_VECTORSTORE\n",
        "    global DISCLOSURES, MATCH_RESULTS, GLOBAL_STATE_INDEX\n",
        "    global RESULTS\n",
        "    global selected_gri, selected_sasb\n",
        "\n",
        "    full_text = None\n",
        "    ESG_CHUNKS = None\n",
        "    ESG_VECTORSTORE = None\n",
        "    DISCLOSURES = []\n",
        "    MATCH_RESULTS = []\n",
        "    GLOBAL_STATE_INDEX = None\n",
        "    RESULTS = {\n",
        "        \"matching\": [],\n",
        "        \"metadata\": {},\n",
        "        \"evaluation\": {},\n",
        "        \"fixes\": {}\n",
        "    }\n",
        "    selected_gri = False\n",
        "    selected_sasb = False\n",
        "\n",
        "    # 🧹 Clear all output directories (full folder deletion)\n",
        "    for folder in [\"faiss_content_index\", \"faiss_global_index\", \"llm_matching_outputs\", \"metadata_outputs\", \"fix_outputs\"]:\n",
        "        try:\n",
        "            if os.path.isdir(folder):\n",
        "                shutil.rmtree(folder)\n",
        "                print(f\"🧹 Cleared directory: {folder}\")\n",
        "            elif os.path.exists(folder):\n",
        "                os.remove(folder)\n",
        "                print(f\"🧹 Removed file: {folder}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not clear {folder}: {e}\")\n",
        "\n",
        "    # 📤 Prompt user to upload\n",
        "    print(\"📤 Please upload your ESG PDF report...\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        return \"❌ No file uploaded.\"\n",
        "\n",
        "    path = list(uploaded.keys())[0]\n",
        "    print(f\"✅ File uploaded: {path}\")\n",
        "\n",
        "    # 🔍 Run extraction\n",
        "    try:\n",
        "        full_text, ESG_CHUNKS = extract_text_docling(path)\n",
        "        if not ESG_CHUNKS or not full_text:\n",
        "            return \"⚠️ Extraction failed: No content extracted from the PDF.\"\n",
        "\n",
        "        ESG_VECTORSTORE = build_esg_report_index(ESG_CHUNKS, rebuild=True)\n",
        "        print(f\"🧠 ESG content index built with {len(ESG_CHUNKS)} chunks.\")\n",
        "\n",
        "        return f\"\"\"✅ ESG report extracted and chunked. {len(ESG_CHUNKS)} chunks generated from: {path}\"\"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Failed to extract PDF: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "# === 2. Select Guideline + Sector ===\n",
        "\n",
        "from langchain.agents import tool\n",
        "from pydantic import BaseModel, Field\n",
        "import os, json, time\n",
        "\n",
        "@tool(return_direct=True)\n",
        "def tool_list_framework_options(_: str = \"\") -> str:\n",
        "    \"\"\"List the available ESG reporting frameworks supported by this pipeline.\"\"\"\n",
        "    return (\n",
        "        \"📚 Available ESG Framework Options:\\n\"\n",
        "        \"1. GRI (Global Reporting Initiative)\\n\"\n",
        "        \"2. SASB (Sustainability Accounting Standards Board)\\n\"\n",
        "        \"3. Both GRI and SASB\\n\\n\"\n",
        "    )\n",
        "\n",
        "class FrameworkInput(BaseModel):\n",
        "    choice: str = Field(..., description=\"1 = GRI only, 2 = SASB only, 3 = Both GRI and SASB\")\n",
        "\n",
        "@tool(args_schema=FrameworkInput, return_direct=True)\n",
        "@track_latency(\"tool_select_framework\")\n",
        "def tool_select_framework(choice: str) -> str:\n",
        "    \"\"\"Step 1: User selects ESG framework(s).\"\"\"\n",
        "    global selected_gri, selected_sasb, DISCLOSURES\n",
        "\n",
        "    if not os.path.exists(\"admin/all_disclosures.json\"):\n",
        "        return \"❌ Disclosures not prepared by admin.\"\n",
        "\n",
        "    all_disclosures = load_admin_disclosures()\n",
        "\n",
        "    selected_gri = selected_sasb = False\n",
        "    DISCLOSURES = []\n",
        "\n",
        "    response = []\n",
        "\n",
        "    if choice == \"1\":\n",
        "        selected_gri = True\n",
        "    elif choice == \"2\":\n",
        "        selected_sasb = True\n",
        "    elif choice == \"3\":\n",
        "        selected_gri = selected_sasb = True\n",
        "    else:\n",
        "        return \"❌ Invalid input. Please choose 1 (GRI), 2 (SASB), or 3 (Both).\"\n",
        "\n",
        "    if selected_gri:\n",
        "        gri_disclosures = [d for d in all_disclosures if \"gri\" in d[\"source\"].lower()]\n",
        "        DISCLOSURES.extend(gri_disclosures)\n",
        "        response.append(f\"✅ Loaded {len(gri_disclosures)} GRI disclosures.\")\n",
        "\n",
        "    if selected_sasb:\n",
        "        response.append(\"📊 SASB selected. Please choose your sector(s) to continue.\")\n",
        "        # 🧠 Inline sector listing\n",
        "        sectors = sorted({\n",
        "            d[\"source\"].split(\"(\", 1)[-1].rstrip(\")\")\n",
        "            for d in all_disclosures if \"sasb\" in d[\"source\"].lower()\n",
        "        })\n",
        "        if sectors:\n",
        "            response.append(\"📋 Available SASB sectors:\\n\" + \"\\n\".join([f\"[{i}] {s}\" for i, s in enumerate(sectors)]))\n",
        "        else:\n",
        "            response.append(\"⚠️ No SASB sectors found.\")\n",
        "\n",
        "    return \"\\n\\n\".join(response)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@tool(return_direct=True)\n",
        "@track_latency(\"tool_list_sasb_sectors\")\n",
        "def tool_list_sasb_sectors() -> str:\n",
        "    \"\"\"Lists available SASB sectors (for use with tool_select_sasb_sectors).\"\"\"\n",
        "    if not os.path.exists(\"admin/all_disclosures.json\"):\n",
        "        return \"❌ Disclosures not found.\"\n",
        "\n",
        "    all_disclosures = load_admin_disclosures()\n",
        "    sectors = sorted({\n",
        "        d[\"source\"].split(\"(\", 1)[-1].rstrip(\")\")\n",
        "        for d in all_disclosures if \"sasb\" in d[\"source\"].lower()\n",
        "    })\n",
        "\n",
        "    if not sectors:\n",
        "        return \"⚠️ No SASB sectors found.\"\n",
        "\n",
        "    return \"📊 Available SASB sectors:\\n\" + \"\\n\".join([f\"[{i}] {name}\" for i, name in enumerate(sectors)])\n",
        "from langchain.agents import tool\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class SasbSectorInput(BaseModel):\n",
        "    sectors: List[int] = Field(..., description=\"List of SASB sector indices. Example: [0, 2, 4]\")\n",
        "\n",
        "@tool(args_schema=SasbSectorInput, return_direct=True)\n",
        "@track_latency(\"tool_select_sasb_sectors\")\n",
        "def tool_select_sasb_sectors(sectors: List[int]) -> str:\n",
        "    \"\"\"Select one or more SASB sectors after enabling SASB framework.\"\"\"\n",
        "    global selected_sasb, DISCLOSURES\n",
        "\n",
        "    if not selected_sasb:\n",
        "        return \"❌ SASB was not selected. Please select the framework first.\"\n",
        "\n",
        "    if not os.path.exists(\"admin/all_disclosures.json\"):\n",
        "        return \"❌ Disclosures not found.\"\n",
        "\n",
        "    all_disclosures = load_admin_disclosures()\n",
        "    all_sectors = sorted({\n",
        "        d[\"source\"].split(\"(\", 1)[-1].rstrip(\")\")\n",
        "        for d in all_disclosures if \"sasb\" in d[\"source\"].lower()\n",
        "    })\n",
        "\n",
        "    try:\n",
        "        selected_names = [all_sectors[i] for i in sectors if 0 <= i < len(all_sectors)]\n",
        "    except Exception:\n",
        "        return \"❌ Invalid sector indices provided.\"\n",
        "\n",
        "    DISCLOSURES.extend([\n",
        "        d for d in all_disclosures\n",
        "        if any(sector.lower() in d[\"source\"].lower() for sector in selected_names)\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"✅ SASB sectors selected: {\", \".join(selected_names)}.\n",
        "📄 Total disclosures loaded: {len(DISCLOSURES)}.\n",
        "\n",
        "Would you like to begin disclosure-to-report semantic matching?\n",
        "You may choose:\n",
        "- 🤖 LLM-based (contextual understanding)\n",
        "- 🧮 Traditional (pattern-based similarity)\n",
        "\"\"\"\n",
        "from langchain_openai import OpenAIEmbeddings  # ✅ Updated import\n",
        "\n",
        "class MatchInput(BaseModel):\n",
        "    method: Optional[str] = Field(None, description=\"Specify 'llm' or 'traditional' for matching method.\")\n",
        "\n",
        "@tool(args_schema=MatchInput, return_direct=True)\n",
        "@track_latency(\"tool_run_matching\")\n",
        "def tool_run_matching(method: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Run disclosure-to-report semantic matching using either 'llm' or 'traditional' methods.\n",
        "\n",
        "    - 'llm': Uses GPT-based contextual understanding (default if selected explicitly).\n",
        "    - 'traditional': Embedding-based cosine similarity (faster, no model usage).\n",
        "\n",
        "    If no method is provided, the agent will request a selection.\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    import json\n",
        "    import os\n",
        "\n",
        "    global ESG_CHUNKS, DISCLOSURES, ESG_VECTORSTORE, MATCH_RESULTS\n",
        "\n",
        "    if not ESG_CHUNKS:\n",
        "        return \"❌ ESG chunks not found. Please upload and extract the ESG report first.\"\n",
        "    if not DISCLOSURES:\n",
        "        return \"❌ Disclosures not loaded. Please select GRI/SASB guideline first.\"\n",
        "\n",
        "    if method is None:\n",
        "        return (\n",
        "            \"❓ Which matching method would you like to use?\\n\\n\"\n",
        "            \"**Options:**\\n\"\n",
        "            \"- `llm`: 🤖 LLM-powered semantic matching (context-aware)\\n\"\n",
        "            \"- `traditional`: 🧮 Embedding-based similarity (faster, no model usage)\\n\\n\"\n",
        "            \"Please type your preferred method to proceed.\"\n",
        "        )\n",
        "\n",
        "    method = method.lower().strip()\n",
        "    if method not in [\"llm\", \"traditional\"]:\n",
        "        return \"❌ Invalid method. Choose either `'llm'` or `'traditional'`.\"\n",
        "\n",
        "    try:\n",
        "        print(\"🔍 Initializing ESG vectorstore for matching...\")\n",
        "        embedding = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "        docs = [Document(page_content=c, metadata={\"chunk_index\": i}) for i, c in enumerate(ESG_CHUNKS)]\n",
        "        ESG_VECTORSTORE = FAISS.from_documents(docs, embedding)\n",
        "\n",
        "        print(f\"🚀 Matching method selected: {method.upper()}\")\n",
        "        MATCH_RESULTS = match_disclosures(\n",
        "            disclosures=DISCLOSURES,\n",
        "            esg_vectorstore=ESG_VECTORSTORE,\n",
        "            top_k=10,\n",
        "            similarity_threshold=1.2,\n",
        "            method=method,\n",
        "            provider_config=providers if method == \"llm\" else None,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        save_matches(MATCH_RESULTS)\n",
        "        if method == \"traditional\":\n",
        "            save_matched_only(MATCH_RESULTS)\n",
        "\n",
        "        RESULTS[\"matching\"] = MATCH_RESULTS\n",
        "        global GLOBAL_STATE_INDEX\n",
        "        GLOBAL_STATE_INDEX = build_runtime_state_index(RESULTS, rebuild=True)\n",
        "\n",
        "        # === Matching Summary ===\n",
        "        if method == \"llm\":\n",
        "            summary_lines = []\n",
        "            total_matched = 0\n",
        "            total_count = 0\n",
        "\n",
        "            for model, matches in MATCH_RESULTS.items():\n",
        "                matched = sum(1 for r in matches if r.get(\"status\") == \"matched\")\n",
        "                count = len(matches)\n",
        "                total_matched += matched\n",
        "                total_count += count\n",
        "                summary_lines.append(f\"🤖 Model **{model.upper()}**: {matched} of {count} matched.\")\n",
        "\n",
        "            summary = \"\\n\".join(summary_lines)\n",
        "            preview = MATCH_RESULTS[next(iter(MATCH_RESULTS))][0] if MATCH_RESULTS else {}\n",
        "\n",
        "        else:\n",
        "            matched = sum(1 for r in MATCH_RESULTS if r.get(\"status\") == \"matched\")\n",
        "            total = len(MATCH_RESULTS)\n",
        "            summary = f\"📊 Traditional matching: {matched} of {total} matched.\"\n",
        "            preview = MATCH_RESULTS[0] if MATCH_RESULTS else {}\n",
        "\n",
        "        preview_clean = {\n",
        "            k: v for k, v in preview.items()\n",
        "            if isinstance(v, (str, int, float, list, dict)) and k != \"matched_chunks\"\n",
        "        }\n",
        "        preview_str = json.dumps(preview_clean, indent=2, ensure_ascii=False) if preview_clean else \"(No preview available)\"\n",
        "\n",
        "        method_description = (\n",
        "            \"🔍 **LLM-based matching**: Uses GPT for context-aware disclosure alignment.\"\n",
        "            if method == \"llm\" else\n",
        "            \"📊 **Traditional matching**: Uses embedding similarity (fast, no LLMs).\"\n",
        "        )\n",
        "\n",
        "        return f\"\"\"✅ Matching completed successfully.\n",
        "\n",
        "{summary}\n",
        "\n",
        "{method_description}\n",
        "\n",
        "📁 Output saved to: {'llm_matching_outputs/' if method == 'llm' else ''}matched_only.json\n",
        "\n",
        "🧾 Preview of first matched disclosure:\n",
        "{preview_str}\n",
        "\"\"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Matching failed due to: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === 4. Metadata Extraction ===\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "class ExtractInput(BaseModel):\n",
        "    extraction_match_source: Optional[str] = Field(\n",
        "        None, description=\"Choose 'llm' for AI-assisted extraction, or 'traditional' for rule-based extraction.\"\n",
        "    )\n",
        "\n",
        "@tool(args_schema=ExtractInput, return_direct=True)\n",
        "@track_latency(\"tool_extract_metadata\")\n",
        "def tool_extract_metadata(extraction_match_source: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Extract metadata for ESG disclosures using either:\n",
        "    - 'llm': AI-assisted extraction based on contextual understanding.\n",
        "    - 'traditional': Pattern- or rule-based methods on pre-matched chunks.\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    import json\n",
        "    import os\n",
        "\n",
        "    global SELECTED_MODELS, providers\n",
        "    if not SELECTED_MODELS:\n",
        "        return \"❌ No models selected. Please run `select_pipeline_models()` first.\"\n",
        "\n",
        "    if extraction_match_source is None:\n",
        "        return (\n",
        "            \"❓ Please specify the match source for metadata extraction:\\n\\n\"\n",
        "            \"- `llm`: Use language models for contextual metadata extraction.\\n\"\n",
        "            \"- `traditional`: Use rule-based pattern extraction on matched chunks.\\n\\n\"\n",
        "        )\n",
        "\n",
        "    extraction_match_source = extraction_match_source.lower().strip()\n",
        "    if extraction_match_source not in [\"llm\", \"traditional\"]:\n",
        "        return \"❌ Invalid choice. Please use 'llm' or 'traditional'.\"\n",
        "\n",
        "    print(f\"📌 Metadata extraction method: {extraction_match_source.upper()}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    if extraction_match_source == \"traditional\":\n",
        "        match_path = \"matched_traditional_best.json\"\n",
        "        if not Path(match_path).exists():\n",
        "            return \"❌ Traditional match results not found.\"\n",
        "\n",
        "        with open(match_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            matched_disclosures = json.load(f)\n",
        "\n",
        "        if not any(d.get(\"matched_chunks\") for d in matched_disclosures):\n",
        "            return \"⚠️ No matched chunks found in traditional results.\"\n",
        "\n",
        "        run_metadata_extraction(matched_disclosures, providers, SELECTED_MODELS, extraction_match_source=\"traditional\")\n",
        "\n",
        "        for model in SELECTED_MODELS:\n",
        "            output_path = f\"metadata_outputs/esg_metadata_{model}_from_traditional.json\"\n",
        "            if Path(output_path).exists():\n",
        "                with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    metadata = json.load(f)\n",
        "                RESULTS[\"metadata\"][model] = metadata\n",
        "                sample = next(iter(metadata.values()), {})\n",
        "\n",
        "                try:\n",
        "                    preview = json.dumps(sample, indent=2, ensure_ascii=False) if sample else \"(no output)\"\n",
        "                except Exception as e:\n",
        "                    preview = f\"[Error rendering preview: {e}]\"\n",
        "\n",
        "                results.append(f\"\"\"✅ Metadata extracted with model: {model}\n",
        "📄 Output file: {output_path}\n",
        "\n",
        "📌 Method: 🔹 Traditional (pattern-based extraction)\n",
        "🔍 Preview of first result:\n",
        "{preview}\n",
        "\"\"\")\n",
        "            else:\n",
        "                results.append(f\"⚠️ No metadata file found for model: {model}\")\n",
        "\n",
        "    elif extraction_match_source == \"llm\":\n",
        "        for model in SELECTED_MODELS:\n",
        "            path = f\"llm_matching_outputs/llm_matched_{model}.json\"\n",
        "            if not Path(path).exists():\n",
        "                results.append(f\"❌ LLM match file not found for model: {model} ({path})\")\n",
        "                continue\n",
        "\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                matched_disclosures = json.load(f)\n",
        "\n",
        "            if not any(d.get(\"matched_chunks\") for d in matched_disclosures):\n",
        "                results.append(f\"⚠️ No matched chunks found for {model}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            run_metadata_extraction(matched_disclosures, providers, [model], extraction_match_source=\"llm\")\n",
        "\n",
        "            output_path = f\"metadata_outputs/esg_metadata_{model}_from_llm.json\"\n",
        "            if Path(output_path).exists():\n",
        "                with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    metadata = json.load(f)\n",
        "                RESULTS[\"metadata\"][model] = metadata\n",
        "                sample = next(iter(metadata.values()), {})\n",
        "\n",
        "                try:\n",
        "                    preview = json.dumps(sample, indent=2, ensure_ascii=False) if sample else \"(no output)\"\n",
        "                except Exception as e:\n",
        "                    preview = f\"[Error rendering preview: {e}]\"\n",
        "\n",
        "                results.append(f\"\"\"✅ Metadata extracted with model: {model}\n",
        "📄 Output file: {output_path}\n",
        "\n",
        "📌 Method: 🤖 LLM-based (AI-assisted extraction)\n",
        "🔍 Preview of first result:\n",
        "{preview}\n",
        "\"\"\")\n",
        "            else:\n",
        "                results.append(f\"⚠️ No metadata file found for model: {model}\")\n",
        "\n",
        "    global GLOBAL_STATE_INDEX\n",
        "    GLOBAL_STATE_INDEX = build_runtime_state_index(RESULTS, rebuild=True)\n",
        "\n",
        "    return \"\\n\\n\".join(results) if results else \"⚠️ No metadata was extracted for any model.\"\n",
        "\n",
        "\n",
        "\n",
        "# === 5. Metadata Evaluation ===@tool(return_direct=True)\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "class EvaluateInput(BaseModel):\n",
        "    evaluation_match_source: Optional[str] = Field(\n",
        "        None, description=\"Choose 'llm' for LLM-evaluated metadata or 'traditional' for rule-based metadata evaluation.\"\n",
        "    )\n",
        "\n",
        "@tool(args_schema=EvaluateInput, return_direct=True)\n",
        "@track_latency(\"tool_evaluate_metadata\")\n",
        "def tool_evaluate_metadata(evaluation_match_source: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate the quality and completeness of extracted ESG metadata.\n",
        "\n",
        "    Supported methods:\n",
        "    - 'llm': Evaluate AI-generated metadata using large language models.\n",
        "    - 'traditional': Evaluate pattern-based metadata using static heuristics.\n",
        "\n",
        "    If unspecified, the method will be inferred automatically based on available files.\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    import json\n",
        "    import os\n",
        "\n",
        "    global SELECTED_MODELS, RESULTS, GLOBAL_STATE_INDEX\n",
        "    if not SELECTED_MODELS:\n",
        "        return \"❌ No models selected. Please run `select_pipeline_models()` first.\"\n",
        "\n",
        "    if \"evaluation\" not in RESULTS or not isinstance(RESULTS[\"evaluation\"], dict):\n",
        "        RESULTS[\"evaluation\"] = {}\n",
        "\n",
        "    if evaluation_match_source is None:\n",
        "        return (\n",
        "            \"❓ Please specify the evaluation method:\\n\\n\"\n",
        "            \"- `llm`: Uses AI models to score ESG metadata coverage and compliance.\\n\"\n",
        "            \"- `traditional`: Applies rule-based logic to structured metadata.\\n\\n\"\n",
        "        )\n",
        "\n",
        "    evaluation_match_source = evaluation_match_source.lower().strip()\n",
        "    if evaluation_match_source not in [\"llm\", \"traditional\"]:\n",
        "        return \"❌ Invalid choice. Please use 'llm' or 'traditional'.\"\n",
        "\n",
        "    print(f\"📌 Evaluation method: {evaluation_match_source.upper()}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model in SELECTED_MODELS:\n",
        "        metadata_path = f\"metadata_outputs/esg_metadata_{model}_from_{evaluation_match_source}.json\"\n",
        "        if not Path(metadata_path).exists():\n",
        "            msg = f\"❌ Metadata not found for model: {model} at {metadata_path}\"\n",
        "            print(msg)\n",
        "            results.append(msg)\n",
        "            continue\n",
        "\n",
        "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        match_path = (\n",
        "            \"matched_only.json\" if evaluation_match_source == \"traditional\"\n",
        "            else f\"llm_matching_outputs/llm_matched_{model}.json\"\n",
        "        )\n",
        "\n",
        "        if not Path(match_path).exists():\n",
        "            msg = f\"❌ Disclosure match file not found: {match_path}\"\n",
        "            print(msg)\n",
        "            results.append(msg)\n",
        "            continue\n",
        "\n",
        "        with open(match_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            matched_disclosures = json.load(f)\n",
        "\n",
        "        output_base = f\"metadata_outputs/metadata_eval_{model}_from_{evaluation_match_source}\"\n",
        "        run_llm_metadata_evaluation(\n",
        "            disclosures=matched_disclosures,\n",
        "            metadata_by_model=metadata,\n",
        "            model_label=model,\n",
        "            model_config=providers[model],\n",
        "            output_prefix=output_base\n",
        "        )\n",
        "\n",
        "        result_path = f\"{output_base}.json\"\n",
        "        if Path(result_path).exists():\n",
        "            with open(result_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                result_data = json.load(f)\n",
        "\n",
        "            RESULTS[\"evaluation\"][model] = result_data\n",
        "            sample = next(iter(result_data.values()), {})\n",
        "\n",
        "            try:\n",
        "                preview = json.dumps(sample, indent=2, ensure_ascii=False) if sample else \"(no result)\"\n",
        "            except Exception as e:\n",
        "                preview = f\"[Error rendering preview: {e}]\"\n",
        "\n",
        "            results.append(f\"\"\"✅ Evaluation completed using model: {model}\n",
        "📄 Output file: {result_path}\n",
        "\n",
        "📌 Method: {'🤖 LLM-based' if evaluation_match_source == 'llm' else '🔍 Traditional rule-based'}\n",
        "📊 Preview of one result:\n",
        "{preview}\n",
        "\"\"\")\n",
        "        else:\n",
        "            results.append(f\"❌ Evaluation file not found: {result_path}\")\n",
        "\n",
        "    GLOBAL_STATE_INDEX = build_runtime_state_index(RESULTS, rebuild=True)\n",
        "\n",
        "    return \"\\n\\n\".join(results) if results else \"⚠️ No evaluation results available for any model.\"\n",
        "\n",
        "\n",
        "# === 6. Fix Suggestions ===\n",
        "class FixInput(BaseModel):\n",
        "    fixes_match_source: Optional[str] = Field(\n",
        "        None, description=\"Choose 'llm' or 'traditional' to indicate the source of evaluated metadata.\"\n",
        "    )\n",
        "\n",
        "@tool(args_schema=FixInput, return_direct=True)\n",
        "@track_latency(\"tool_generate_fixes\")\n",
        "def tool_generate_fixes(fixes_match_source: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Generate ESG fix suggestions for incomplete or non-compliant disclosures based on evaluated metadata.\n",
        "\n",
        "    Match source options:\n",
        "    - 'llm': Uses AI-evaluated metadata to generate disclosure improvement suggestions.\n",
        "    - 'traditional': Uses heuristic-based evaluations for generating suggestions.\n",
        "\n",
        "    If not provided, the tool will prompt for input.\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    import json\n",
        "    import os\n",
        "\n",
        "    fix_dir = \"fix_outputs\"\n",
        "    meta_dir = \"metadata_outputs\"\n",
        "    os.makedirs(fix_dir, exist_ok=True)\n",
        "    os.makedirs(meta_dir, exist_ok=True)\n",
        "\n",
        "    global SELECTED_MODELS, RESULTS, GLOBAL_STATE_INDEX\n",
        "    if not SELECTED_MODELS:\n",
        "        return \"❌ No models selected. Please run `select_pipeline_models()` first.\"\n",
        "\n",
        "    if \"fixes\" not in RESULTS or not isinstance(RESULTS[\"fixes\"], dict):\n",
        "        RESULTS[\"fixes\"] = {}\n",
        "\n",
        "    if not os.path.exists(\"admin/all_disclosures.json\"):\n",
        "        return \"❌ Disclosure rules not found in the admin directory.\"\n",
        "\n",
        "    if fixes_match_source is None:\n",
        "        return (\n",
        "            \"❓ Which evaluation result would you like to use to generate fixes?\\n\\n\"\n",
        "            \"**Options:**\\n\"\n",
        "            \"- `llm`: 🤖 LLM-evaluated metadata\\n\"\n",
        "            \"- `traditional`: 📘 Rule-based evaluation\\n\\n\"\n",
        "            \"Please provide your choice.\"\n",
        "        )\n",
        "\n",
        "    fixes_match_source = fixes_match_source.lower().strip()\n",
        "    if fixes_match_source not in [\"llm\", \"traditional\"]:\n",
        "        return \"❌ Invalid source. Use either `'llm'` or `'traditional'`.\"\n",
        "\n",
        "    with open(\"admin/all_disclosures.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        disclosures = json.load(f)\n",
        "\n",
        "    rule_vectorstore = build_disclosure_rule_vectorstore(disclosures)\n",
        "    results = []\n",
        "\n",
        "    for model in SELECTED_MODELS:\n",
        "        eval_path = Path(f\"{meta_dir}/metadata_eval_{model}_from_{fixes_match_source}.json\")\n",
        "        if not eval_path.exists():\n",
        "            msg = f\"❌ Evaluation results not found for model: {model} at {eval_path}\"\n",
        "            print(msg)\n",
        "            results.append(msg)\n",
        "            continue\n",
        "\n",
        "        with open(eval_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            compliance_report = list(json.load(f).values())\n",
        "\n",
        "        clean_report = []\n",
        "        for r in compliance_report:\n",
        "            if isinstance(r, dict):\n",
        "                r[\"rating\"] = r.get(\"rating\", \"❓\")\n",
        "                r[\"reason\"] = r.get(\"reason\") or r.get(\"justification\", \"No reason provided\")\n",
        "                r[\"disclosure_id\"] = r.get(\"disclosure_id\") or r.get(\"id\") or \"[unknown]\"\n",
        "                clean_report.append(r)\n",
        "\n",
        "        try:\n",
        "            raw_fixes = generate_esg_fixes_with_retrieval_multi(\n",
        "                compliance_report=clean_report,\n",
        "                rule_vectorstore=rule_vectorstore,\n",
        "                model_configs=providers,\n",
        "                selected_models=[model],\n",
        "                return_raw=True,\n",
        "                fixes_match_source=fixes_match_source\n",
        "            )\n",
        "        except Exception as e:\n",
        "            error = f\"❌ Error generating fixes for model '{model}': {e}\"\n",
        "            print(error)\n",
        "            results.append(error)\n",
        "            continue\n",
        "\n",
        "        output_path = f\"{fix_dir}/esg_fixes_{model}_from_{fixes_match_source}.json\"\n",
        "        if Path(output_path).exists():\n",
        "            try:\n",
        "                with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    result_data = json.load(f)\n",
        "                RESULTS[\"fixes\"][model] = result_data\n",
        "                sample = result_data[0] if result_data else {}\n",
        "\n",
        "                try:\n",
        "                    preview = json.dumps(sample, indent=2, ensure_ascii=False) if sample else \"(no fix generated)\"\n",
        "                except Exception as e:\n",
        "                    preview = f\"[Error rendering preview: {e}]\"\n",
        "\n",
        "                source_readable = \"🤖 LLM-based\" if fixes_match_source == \"llm\" else \"📘 Traditional\"\n",
        "                results.append(f\"\"\"✅ Fix suggestions generated using model: {model}\n",
        "📄 Output file: {output_path}\n",
        "\n",
        "🔧 Method: {source_readable}\n",
        "🛠️ Preview of first fix:\n",
        "{preview}\n",
        "\"\"\")\n",
        "            except Exception as e:\n",
        "                results.append(f\"❌ Failed to load saved fixes for model {model}: {e}\")\n",
        "        else:\n",
        "            results.append(f\"⚠️ Fix generation completed, but no output file found for model: {model}\")\n",
        "\n",
        "    GLOBAL_STATE_INDEX = build_runtime_state_index(RESULTS, rebuild=True)\n",
        "\n",
        "    return \"\\n\\n\".join(results) if results else \"⚠️ No fix suggestions were generated for any model.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === Utility Tools ===\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "class QueryESGStateInput(BaseModel):\n",
        "    question: str = Field(..., description=\"The user’s natural language query about ESG pipeline outputs.\")\n",
        "    section: Optional[str] = Field(None, description=\"One of: matching, metadata, evaluation, fixes\")\n",
        "    model: Optional[str] = Field(None, description=\"Optional model filter: gpt, claude, or gemini\")\n",
        "    top_k: int = Field(30, description=\"Number of results to retrieve (default: 30)\")\n",
        "\n",
        "\n",
        "@tool(args_schema=QueryESGStateInput, return_direct=True)\n",
        "def query_esg_state(question: str, section: Optional[str] = None, model: Optional[str] = None, top_k: int = 30) -> str:\n",
        "    \"\"\"Semantic query over ESG pipeline outputs (matched disclosures, metadata, evaluations, fixes).\"\"\"\n",
        "    import re\n",
        "    global GLOBAL_STATE_INDEX\n",
        "    if not GLOBAL_STATE_INDEX:\n",
        "        return \"❌ ESG state index is not initialized. Please run earlier steps to populate outputs first.\"\n",
        "\n",
        "    # === Heuristic Section Inference ===\n",
        "    if not section:\n",
        "        lowered = question.lower()\n",
        "        if \"match\" in lowered or \"alignment\" in lowered:\n",
        "            section = \"matching\"\n",
        "        elif \"metadata\" in lowered or \"extract\" in lowered:\n",
        "            section = \"metadata\"\n",
        "        elif \"evaluation\" in lowered or \"compliance\" in lowered:\n",
        "            section = \"evaluation\"\n",
        "        elif \"fix\" in lowered or \"improve\" in lowered:\n",
        "            section = \"fixes\"\n",
        "\n",
        "    # === Normalize Model Name ===\n",
        "    model_alias_map = {\n",
        "        \"gpt\": \"gpt\", \"gpt4\": \"gpt\", \"gpt-4\": \"gpt\",\n",
        "        \"claude\": \"claude\", \"opus\": \"claude\",\n",
        "        \"gemini\": \"gemini\", \"gemini-pro\": \"gemini\"\n",
        "    }\n",
        "    normalized_model = model_alias_map.get(model.lower(), model.lower()) if model else None\n",
        "\n",
        "    metadata_filter = {}\n",
        "    if section:\n",
        "        metadata_filter[\"section\"] = section\n",
        "    if normalized_model:\n",
        "        metadata_filter[\"model\"] = normalized_model\n",
        "\n",
        "    retriever = GLOBAL_STATE_INDEX.as_retriever(\n",
        "        search_kwargs={\"k\": max(top_k, 5)},\n",
        "        filter=metadata_filter if metadata_filter else None\n",
        "    )\n",
        "\n",
        "    results = retriever.invoke(question)\n",
        "    if not results:\n",
        "        tip = \"You may need to run earlier tools (e.g., matching, metadata extraction) before querying this section.\"\n",
        "        return f\"⚠️ No relevant results found for your query.\\n💡 Tip: {tip}\"\n",
        "\n",
        "    response_lines = []\n",
        "    for idx, doc in enumerate(results, 1):\n",
        "        sec = doc.metadata.get(\"section\", \"\")\n",
        "        disclosure_id = doc.metadata.get(\"id\", \"\")\n",
        "        rating = doc.metadata.get(\"rating\", \"\")\n",
        "        reason = doc.metadata.get(\"reason\", \"\")\n",
        "        model_name = doc.metadata.get(\"model\", \"unknown\").upper()\n",
        "        content = doc.page_content.strip()\n",
        "\n",
        "        if sec == \"matching\":\n",
        "            try:\n",
        "                parsed = json.loads(content)\n",
        "                if parsed.get(\"status\") != \"matched\":\n",
        "                    continue\n",
        "                formatted = json.dumps(parsed, indent=2, ensure_ascii=False)\n",
        "                response_lines.append(\n",
        "                    f\"📌 Match {idx} — Section: {sec.upper()} | Model: {model_name}\\n\"\n",
        "                    f\"```json\\n{formatted}\\n```\"\n",
        "                )\n",
        "            except Exception:\n",
        "                continue\n",
        "        else:\n",
        "            preview = content[:300] + (\"...\" if len(content) > 300 else \"\")\n",
        "            response_lines.append(\n",
        "                f\"📌 Result {idx} — Section: {sec.upper()} | Model: {model_name}\\n\"\n",
        "                f\"📄 Disclosure: {disclosure_id or '[unknown]'}\\n\"\n",
        "                f\"📊 Rating: {rating or '[n/a]'}\\n\"\n",
        "                f\"📝 Reason: {reason or '[none]'}\\n\"\n",
        "                f\"🔍 Content Preview: {preview}\"\n",
        "            )\n",
        "\n",
        "    summary_note = (\n",
        "        f\"<thinking>Query retrieved {len(response_lines)} result(s) \"\n",
        "        f\"from section='{section or 'any'}', model='{model or 'any'}'.</thinking>\"\n",
        "    )\n",
        "\n",
        "    return summary_note + \"\\n\\n\" + \"\\n\\n\".join(response_lines)\n",
        "@tool(args_schema=QueryESGStateInput, return_direct=True)\n",
        "def query_esg_report(question: str, top_k: int = 20) -> str:\n",
        "    \"\"\"Semantic query over the full content of the uploaded ESG report.\"\"\"\n",
        "    global ESG_VECTORSTORE\n",
        "    if not ESG_VECTORSTORE:\n",
        "        return \"❌ ESG content index is not initialized. Please upload and extract an ESG report first.\"\n",
        "\n",
        "    if not question.strip():\n",
        "        return \"⚠️ Your query appears empty. Please enter a meaningful question about the ESG content.\"\n",
        "\n",
        "    # Run semantic retrieval\n",
        "    docs = ESG_VECTORSTORE.similarity_search(question, k=top_k)\n",
        "    if not docs:\n",
        "        return \"⚠️ No relevant content found for your query within the report.\"\n",
        "\n",
        "    response_lines = []\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        content = doc.page_content.strip()\n",
        "        preview = content[:400] + (\"...\" if len(content) > 400 else \"\")\n",
        "        chunk_id = doc.metadata.get(\"chunk_index\", f\"{i - 1}\")\n",
        "\n",
        "        response_lines.append(\n",
        "            f\"🔹 Match {i} — Chunk ID: {chunk_id}\\n\"\n",
        "            f\"```text\\n{preview}\\n```\"\n",
        "        )\n",
        "\n",
        "    return (\n",
        "        f\"<thinking>Top {len(response_lines)} relevant chunk(s) retrieved from the ESG report.</thinking>\\n\\n\"\n",
        "        + \"\\n\\n\".join(response_lines)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "wNUmCciWxs6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestration Agent with token usage tracker"
      ],
      "metadata": {
        "id": "IJKpIulxepqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import time  # ⏱️ For measuring latency\n",
        "\n",
        "# === Ensure Global Latency Field Exists ===\n",
        "if \"total_latency_sec\" not in LLM_COST_LOG:\n",
        "    LLM_COST_LOG[\"total_latency_sec\"] = 0.0\n",
        "\n",
        "# === Display Helper ===\n",
        "def display_token_usage(current, total):\n",
        "    html = f\"\"\"\n",
        "    <div style=\"font-family:Segoe UI, sans-serif; font-size:15px; margin-top:20px;\">\n",
        "        <div style=\"background:#f8f8f8; border-left:4px solid #4CAF50; padding:12px; border-radius:8px;\">\n",
        "            <b>🧾 Token Usage:</b><br>\n",
        "            <span style=\"color:#333;\">Prompt Tokens:</span> {current['prompt_tokens']}<br>\n",
        "            <span style=\"color:#333;\">Completion Tokens:</span> {current['completion_tokens']}<br>\n",
        "            <span style=\"color:#333;\">Total Tokens:</span> {current['total_tokens']}<br>\n",
        "            <span style=\"color:#333;\">Cost:</span> ${current['total_cost']:.6f}<br>\n",
        "            <span style=\"color:#333;\">⏱️ Response Time:</span> {current['latency_sec']:.2f} seconds\n",
        "        </div>\n",
        "        <div style=\"margin-top:12px; background:#f1f1f1; border-left:4px solid #2196F3; padding:12px; border-radius:8px;\">\n",
        "            <b>📊 Cumulative Total:</b><br>\n",
        "            <span style=\"color:#333;\">Prompt Tokens:</span> {total['prompt_tokens']}<br>\n",
        "            <span style=\"color:#333;\">Completion Tokens:</span> {total['completion_tokens']}<br>\n",
        "            <span style=\"color:#333;\">Total Tokens:</span> {total['total_tokens']}<br>\n",
        "            <span style=\"color:#333;\">Total Cost:</span> ${total['total_cost']:.6f}<br>\n",
        "            <span style=\"color:#333;\">⏱️ Total Latency:</span> {total['total_latency_sec']:.2f} seconds\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "\n",
        "# === Main Follow-up Chat Widget ===# === Main Follow-up Chat Widget ===\n",
        "def enable_followup_input(task_name=\"ESG Report\"):\n",
        "    input_box = widgets.Text(\n",
        "        placeholder=f\"Ask a question about your '{task_name}'...\",\n",
        "        layout=widgets.Layout(width=\"75%\", padding=\"6px\")\n",
        "    )\n",
        "\n",
        "    submit_button = widgets.Button(\n",
        "        description=\"Submit\",\n",
        "        button_style=\"primary\",\n",
        "        layout=widgets.Layout(width=\"80px\")\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output(layout=widgets.Layout(\n",
        "        border='1px solid #ccc', padding=\"10px\", max_height=\"400px\", overflow_y=\"auto\")\n",
        "    )\n",
        "\n",
        "    def on_submit(_):\n",
        "        user_question = input_box.value.strip()\n",
        "        input_box.value = \"\"\n",
        "\n",
        "        if not user_question:\n",
        "            with output_area:\n",
        "                clear_output()\n",
        "                display(HTML(\"<span style='color:#d33;'>⚠️ Please type a question before submitting.</span>\"))\n",
        "            return\n",
        "\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            display(HTML(f\"\"\"\n",
        "                <div style=\"font-family:Segoe UI, sans-serif; margin-bottom:10px;\">\n",
        "                    <b>📨 User Input:</b>\n",
        "                    <div style=\"background:#f1f1f1; padding:8px 10px; border-radius:6px; margin-top:5px;\">\n",
        "                        {user_question}\n",
        "                    </div>\n",
        "                    <div style=\"color:#888; margin-top:8px;\">⏳ Processing your request...</div>\n",
        "                </div>\n",
        "            \"\"\"))\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            with get_openai_callback() as cb:\n",
        "                result = agent.invoke({\"input\": user_question})\n",
        "                latency = round(time.time() - start_time, 2)\n",
        "\n",
        "                usage = {\n",
        "                    \"prompt_tokens\": cb.prompt_tokens,\n",
        "                    \"completion_tokens\": cb.completion_tokens,\n",
        "                    \"total_tokens\": cb.total_tokens,\n",
        "                    \"total_cost\": cb.total_cost,\n",
        "                    \"latency_sec\": latency\n",
        "                }\n",
        "\n",
        "                # ✅ Update cumulative tracker\n",
        "                LLM_COST_LOG[\"prompt_tokens\"] += cb.prompt_tokens\n",
        "                LLM_COST_LOG[\"completion_tokens\"] += cb.completion_tokens\n",
        "                LLM_COST_LOG[\"total_tokens\"] += cb.total_tokens\n",
        "                LLM_COST_LOG[\"total_cost\"] += cb.total_cost\n",
        "                LLM_COST_LOG[\"total_latency_sec\"] += latency\n",
        "                log_entry = {\n",
        "                    \"question\": user_question,\n",
        "                    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    \"prompt_tokens\": cb.prompt_tokens,\n",
        "                    \"completion_tokens\": cb.completion_tokens,\n",
        "                    \"total_tokens\": cb.total_tokens,\n",
        "                    \"total_cost\": round(cb.total_cost, 6),\n",
        "                    \"latency_sec\": latency\n",
        "                }\n",
        "                with open(\"/content/agent_logs.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(json.dumps(log_entry) + \"\\n\")\n",
        "\n",
        "\n",
        "            output_text = result.get(\"output\", \"🤷 No answer found.\")\n",
        "            output_text_html = output_text.replace('\\n', '<br/>')\n",
        "\n",
        "            with output_area:\n",
        "                clear_output()\n",
        "                display(HTML(f\"\"\"\n",
        "                    <div style=\"font-family:Segoe UI, sans-serif;\">\n",
        "                        <b>📨  User Input:</b>\n",
        "                        <div style=\"background:#f1f1f1; padding:8px 10px; border-radius:6px; margin-top:5px;\">\n",
        "                            {user_question}\n",
        "                        </div>\n",
        "\n",
        "                        <b style=\"display:block; margin-top:15px;\">🧠 Response:</b>\n",
        "                        <div style=\"background:#f9f9f9; border-left:4px solid #4CAF50; padding:10px; border-radius:6px; margin-top:5px;\">\n",
        "                            {output_text_html}\n",
        "                        </div>\n",
        "                    </div>\n",
        "                \"\"\"))\n",
        "                display_token_usage(usage, LLM_COST_LOG)\n",
        "\n",
        "        except Exception as e:\n",
        "            with output_area:\n",
        "                clear_output()\n",
        "                display(HTML(f\"<div style='color:#c00;'>❌ Error: {e}</div>\"))\n",
        "\n",
        "    submit_button.on_click(on_submit)\n",
        "    input_box.on_submit(on_submit)  # ✅ ENTER key submits\n",
        "\n",
        "    display(widgets.VBox([\n",
        "        widgets.HBox([input_box, submit_button]),\n",
        "        output_area\n",
        "    ]))\n"
      ],
      "metadata": {
        "id": "3dg2oWZnhaJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
        "from langchain.schema.messages import SystemMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# === Global Token Tracker ===\n",
        "LLM_COST_LOG = {\n",
        "    \"prompt_tokens\": 0,\n",
        "    \"completion_tokens\": 0,\n",
        "    \"total_tokens\": 0,\n",
        "    \"total_cost\": 0.0,\n",
        "    \"total_latency_sec\": 0.0\n",
        "}\n",
        "\n",
        "# === Function to Show Cumulative Token Summary ===\n",
        "def print_agent_token_usage():\n",
        "    print(\"🧾 Agent Token Usage Summary:\")\n",
        "    print(f\"🔹 Prompt Tokens:     {LLM_COST_LOG['prompt_tokens']}\")\n",
        "    print(f\"🔹 Completion Tokens: {LLM_COST_LOG['completion_tokens']}\")\n",
        "    print(f\"🔹 Total Tokens:      {LLM_COST_LOG['total_tokens']}\")\n",
        "    print(f\"💵 Total Cost (USD):  ${LLM_COST_LOG['total_cost']:.6f}\")\n",
        "\n",
        "# === Create plain LLM ===\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.2)\n",
        "\n",
        "# === Tool Registry ===\n",
        "tools = [\n",
        "    tool_upload_esg_pdf,\n",
        "    tool_list_framework_options,\n",
        "    tool_select_framework,\n",
        "    tool_list_sasb_sectors,\n",
        "    tool_select_sasb_sectors,\n",
        "    tool_run_matching,\n",
        "    tool_extract_metadata,\n",
        "    tool_evaluate_metadata,\n",
        "    tool_generate_fixes,\n",
        "    query_esg_state,\n",
        "    query_esg_report\n",
        "]\n",
        "\n",
        "# === Prompt for Orchestration ===\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"Always start with upload tool\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "# === Build Agent ===\n",
        "agent_fn = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "agent = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent_fn,\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=False,\n",
        "    return_intermediate_steps=False,\n",
        "    handle_parsing_errors=True,\n",
        "    stream_runnable=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "_fQhElTxdcZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_pipeline_models():\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "    global SELECTED_MODELS\n",
        "    if SELECTED_MODELS is None:\n",
        "        SELECTED_MODELS = []\n",
        "\n",
        "    description = \"\"\"\n",
        "### 💡 ESG Pipeline – LLM Model Selection\n",
        "\n",
        "This ESG pipeline uses language models (LLMs) in the following steps:\n",
        "\n",
        "1. **Metadata Extraction** – Extract KPIs and narrative content\n",
        "2. **Compliance Evaluation** – Check if disclosures are satisfied\n",
        "3. **Fix Generation** – Suggest missing elements or improvements\n",
        "\n",
        "Please choose one or more LLMs to be used throughout these steps:\n",
        "\"\"\"\n",
        "\n",
        "    display(Markdown(description))\n",
        "\n",
        "    options = [\"gpt\", \"claude\", \"gemini\"]\n",
        "    checkboxes = [\n",
        "        widgets.Checkbox(value=(opt in SELECTED_MODELS or (not SELECTED_MODELS and opt == \"gpt\")), description=opt)\n",
        "        for opt in options\n",
        "    ]\n",
        "    submit_button = widgets.Button(description=\"Confirm Selection\", button_style=\"success\")\n",
        "    output = widgets.Output()\n",
        "    def on_submit(_):\n",
        "        selected = [cb.description for cb in checkboxes if cb.value]\n",
        "        with output:\n",
        "            clear_output()\n",
        "            if not selected:\n",
        "                print(\"❌ Please select at least one model.\")\n",
        "            else:\n",
        "                SELECTED_MODELS.clear()\n",
        "                SELECTED_MODELS.extend(selected)\n",
        "                print(f\"✅ Models selected: {', '.join(SELECTED_MODELS)}\")\n",
        "\n",
        "    submit_button.on_click(on_submit)\n",
        "    display(widgets.VBox(checkboxes + [submit_button, output]))\n",
        "\n",
        "select_pipeline_models()"
      ],
      "metadata": {
        "id": "bMng2rh9V3U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "start_time = time.time()\n",
        "with get_openai_callback() as cb:\n",
        "    result = agent.invoke({\"input\": \"Let's begin processing an ESG report.\"})\n",
        "    latency = time.time() - start_time\n",
        "\n",
        "    print(result[\"output\"])\n",
        "\n",
        "    # Update global tracker\n",
        "    LLM_COST_LOG[\"prompt_tokens\"] += cb.prompt_tokens\n",
        "    LLM_COST_LOG[\"completion_tokens\"] += cb.completion_tokens\n",
        "    LLM_COST_LOG[\"total_tokens\"] += cb.total_tokens\n",
        "    LLM_COST_LOG[\"total_cost\"] += cb.total_cost\n",
        "    LLM_COST_LOG[\"total_latency_sec\"] += latency\n",
        "    log_entry = {\n",
        "    \"question\": \"Let's begin processing an ESG report.\",\n",
        "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"prompt_tokens\": cb.prompt_tokens,\n",
        "    \"completion_tokens\": cb.completion_tokens,\n",
        "    \"total_tokens\": cb.total_tokens,\n",
        "    \"total_cost\": round(cb.total_cost, 6),\n",
        "    \"latency_sec\": round(latency, 2)\n",
        "}\n",
        "with open(\"/content/agent_logs.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "    f.write(json.dumps(log_entry) + \"\\n\")\n",
        "\n",
        "\n",
        "    print(\"🧾 Initial Agent Token Usage:\")\n",
        "    print(f\"🔹 Prompt: {cb.prompt_tokens}, Completion: {cb.completion_tokens}, Total: {cb.total_tokens}\")\n",
        "    print(f\"💵 Cost: ${cb.total_cost:.6f}\")\n",
        "    print(f\"⏱️ Latency: {latency:.2f} seconds\")\n",
        "\n",
        "# Enable interaction\n",
        "enable_followup_input(\"ESG Report\")\n"
      ],
      "metadata": {
        "id": "P6o4CoG4dfns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}